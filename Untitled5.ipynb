{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092000cc-3cd1-4247-8311-cfa80a18b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 16:10:24.259670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740615024.274419   19896 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740615024.279174   19896 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 16:10:24.294484: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class MediaPipeToBVH:\n",
    "    def __init__(self, fps: int = 30, scale: float = 100.0):\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.fps = fps\n",
    "        self.scale = scale\n",
    "        \n",
    "        # Define the BVH skeleton hierarchy\n",
    "        self.joint_hierarchy = {\n",
    "            \"Hips\": [\"Spine\", \"LeftUpLeg\", \"RightUpLeg\"],\n",
    "            \"Spine\": [\"Spine1\"],\n",
    "            \"Spine1\": [\"Neck\", \"LeftShoulder\", \"RightShoulder\"],\n",
    "            \"Neck\": [\"Head\"],\n",
    "            \"Head\": [\"End_Head\"],\n",
    "            \"LeftShoulder\": [\"LeftArm\"],\n",
    "            \"LeftArm\": [\"LeftForeArm\"],\n",
    "            \"LeftForeArm\": [\"LeftHand\"],\n",
    "            \"LeftHand\": [\"End_LeftHand\"],\n",
    "            \"RightShoulder\": [\"RightArm\"],\n",
    "            \"RightArm\": [\"RightForeArm\"],\n",
    "            \"RightForeArm\": [\"RightHand\"],\n",
    "            \"RightHand\": [\"End_RightHand\"],\n",
    "            \"LeftUpLeg\": [\"LeftLeg\"],\n",
    "            \"LeftLeg\": [\"LeftFoot\"],\n",
    "            \"LeftFoot\": [\"End_LeftFoot\"],\n",
    "            \"RightUpLeg\": [\"RightLeg\"],\n",
    "            \"RightLeg\": [\"RightFoot\"],\n",
    "            \"RightFoot\": [\"End_RightFoot\"]\n",
    "        }\n",
    "        \n",
    "        # MediaPipe landmark indices mapping to BVH joints\n",
    "        self.landmark_to_joint = {\n",
    "            # Main body\n",
    "            \"Hips\": {\"indices\": [23, 24], \"weights\": [0.5, 0.5]},\n",
    "            \"Spine\": {\"indices\": [23, 24, 11, 12], \"weights\": [0.25, 0.25, 0.25, 0.25]},\n",
    "            \"Spine1\": {\"indices\": [11, 12], \"weights\": [0.5, 0.5]},\n",
    "            \"Neck\": {\"indices\": [11, 12], \"weights\": [0.5, 0.5]},\n",
    "            \"Head\": {\"indices\": [0], \"weights\": [1.0]},\n",
    "            \n",
    "            # Left arm\n",
    "            \"LeftShoulder\": {\"indices\": [11], \"weights\": [1.0]},\n",
    "            \"LeftArm\": {\"indices\": [13], \"weights\": [1.0]},\n",
    "            \"LeftForeArm\": {\"indices\": [15], \"weights\": [1.0]},\n",
    "            \"LeftHand\": {\"indices\": [15, 17, 19], \"weights\": [0.4, 0.3, 0.3]},\n",
    "            \n",
    "            # Right arm\n",
    "            \"RightShoulder\": {\"indices\": [12], \"weights\": [1.0]},\n",
    "            \"RightArm\": {\"indices\": [14], \"weights\": [1.0]},\n",
    "            \"RightForeArm\": {\"indices\": [16], \"weights\": [1.0]},\n",
    "            \"RightHand\": {\"indices\": [16, 18, 20], \"weights\": [0.4, 0.3, 0.3]},\n",
    "            \n",
    "            # Left leg - Properly weighted to match visual\n",
    "            \"LeftUpLeg\": {\"indices\": [23], \"weights\": [1.0]},\n",
    "            \"LeftLeg\": {\"indices\": [25], \"weights\": [1.0]},\n",
    "            \"LeftFoot\": {\"indices\": [27, 31], \"weights\": [0.8, 0.2]},\n",
    "            \n",
    "            # Right leg - Properly weighted to match visual\n",
    "            \"RightUpLeg\": {\"indices\": [24], \"weights\": [1.0]},\n",
    "            \"RightLeg\": {\"indices\": [26], \"weights\": [1.0]},\n",
    "            \"RightFoot\": {\"indices\": [28, 32], \"weights\": [0.8, 0.2]}\n",
    "        }\n",
    "        \n",
    "        # End site offsets (fixed lengths for end joints)\n",
    "        self.end_site_offsets = {\n",
    "            \"End_Head\": (0, 15, 0),         # Up from head\n",
    "            \"End_LeftHand\": (-5, 0, 0),     # Left from hand\n",
    "            \"End_RightHand\": (5, 0, 0),     # Right from hand\n",
    "            \"End_LeftFoot\": (0, -5, 5),     # Down and forward from foot\n",
    "            \"End_RightFoot\": (0, -5, 5)     # Down and forward from foot\n",
    "        }\n",
    "        \n",
    "        # Initialize pose tracking\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            smooth_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        # Default orientation is facing forward (-Z)\n",
    "        self.model_forward = -1  # -1 means model faces -Z, 1 means model faces +Z\n",
    "\n",
    "    def process_video(self, video_path, visualize=False, max_frames=None):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file {video_path}\")\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if max_frames and max_frames < total_frames:\n",
    "            total_frames = max_frames\n",
    "            \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        print(f\"Video has {total_frames} frames at {fps} FPS\")\n",
    "        \n",
    "        # Optional visualization video\n",
    "        out = None\n",
    "        if visualize:\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter('pose_tracking.mp4', fourcc, fps, (width, height))\n",
    "        \n",
    "        frames = []\n",
    "        frame_count = 0\n",
    "        last_good_landmarks = None\n",
    "        \n",
    "        while frame_count < total_frames:\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            # Progress indicator\n",
    "            if frame_count % max(1, total_frames // 10) == 0:\n",
    "                print(f\"Processing frame {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%)\")\n",
    "            \n",
    "            # Convert to RGB for MediaPipe\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = self.pose.process(image_rgb)\n",
    "            \n",
    "            if visualize and results.pose_landmarks:\n",
    "                # Draw pose landmarks on image\n",
    "                annotated_image = image.copy()\n",
    "                self.mp_drawing.draw_landmarks(\n",
    "                    annotated_image, \n",
    "                    results.pose_landmarks,\n",
    "                    self.mp_pose.POSE_CONNECTIONS\n",
    "                )\n",
    "                out.write(annotated_image)\n",
    "            \n",
    "            if results.pose_world_landmarks:\n",
    "                # Store landmarks\n",
    "                landmarks_dict = {}\n",
    "                for i, landmark in enumerate(results.pose_world_landmarks.landmark):\n",
    "                    landmarks_dict[i] = {\n",
    "                        'x': landmark.x,\n",
    "                        'y': landmark.y,\n",
    "                        'z': landmark.z,\n",
    "                        'visibility': landmark.visibility\n",
    "                    }\n",
    "                frames.append(landmarks_dict)\n",
    "                last_good_landmarks = landmarks_dict\n",
    "            else:\n",
    "                # If no pose detected, use last good landmarks\n",
    "                if last_good_landmarks:\n",
    "                    frames.append(last_good_landmarks.copy())\n",
    "                else:\n",
    "                    print(f\"Warning: No pose detected in frame {frame_count} and no previous landmarks available\")\n",
    "        \n",
    "        if visualize and out:\n",
    "            out.release()\n",
    "            \n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) == 0:\n",
    "            raise ValueError(\"No pose data detected in the video\")\n",
    "        \n",
    "        print(f\"Extracted pose data from {len(frames)} frames\")\n",
    "        return frames\n",
    "\n",
    "    def calculate_joint_positions(self, frames, flip_forward=True):\n",
    "        \"\"\"Calculate BVH joint positions from MediaPipe landmarks\"\"\"\n",
    "        joint_positions = []\n",
    "        \n",
    "        for frame_idx, landmarks in enumerate(frames):\n",
    "            frame_positions = {}\n",
    "            \n",
    "            # Calculate position for each joint\n",
    "            for joint_name, mapping in self.landmark_to_joint.items():\n",
    "                indices = mapping[\"indices\"]\n",
    "                weights = mapping[\"weights\"]\n",
    "                \n",
    "                # Calculate weighted average position\n",
    "                x = sum(landmarks[idx]['x'] * weights[i] for i, idx in enumerate(indices))\n",
    "                y = sum(landmarks[idx]['y'] * weights[i] for i, idx in enumerate(indices))\n",
    "                z = sum(landmarks[idx]['z'] * weights[i] for i, idx in enumerate(indices))\n",
    "                \n",
    "                # Convert MediaPipe coordinates to standard BVH coordinates:\n",
    "                # MediaPipe: Y is up, X is right, Z is toward camera\n",
    "                # Standard BVH: Y is up, X is right, Z is forward (away from camera)\n",
    "                \n",
    "                # For 180° rotation, we negate both X and Z\n",
    "                if flip_forward:\n",
    "                    frame_positions[joint_name] = {\n",
    "                        'x': -x * self.scale,       # Flip X to rotate 180°\n",
    "                        'y': y * self.scale,        # Y stays the same\n",
    "                        'z': z * self.scale         # Z is toward camera (not flipped to rotate 180°)\n",
    "                    }\n",
    "                else:\n",
    "                    frame_positions[joint_name] = {\n",
    "                        'x': x * self.scale,        # X stays the same\n",
    "                        'y': y * self.scale,        # Y stays the same\n",
    "                        'z': -z * self.scale        # Z is flipped to make forward away from camera\n",
    "                    }\n",
    "            \n",
    "            joint_positions.append(frame_positions)\n",
    "        \n",
    "        return joint_positions\n",
    "\n",
    "    def detect_facing_direction(self, positions):\n",
    "        \"\"\"Detect which way the person is predominantly facing\"\"\"\n",
    "        if len(positions) < 10:  # Need at least a few frames to be reliable\n",
    "            return False  # Default to not flipping\n",
    "            \n",
    "        # Look at the hip-to-shoulder relationship to determine facing\n",
    "        facing_camera_frames = 0\n",
    "        facing_away_frames = 0\n",
    "        \n",
    "        for frame in positions[:min(30, len(positions))]:  # Check first 30 frames (or all if fewer)\n",
    "            if \"Hips\" in frame and \"Spine1\" in frame:\n",
    "                # Z distance from hips to shoulders\n",
    "                z_diff = frame[\"Spine1\"][\"z\"] - frame[\"Hips\"][\"z\"]\n",
    "                \n",
    "                if z_diff > 0:\n",
    "                    facing_camera_frames += 1\n",
    "                else:\n",
    "                    facing_away_frames += 1\n",
    "        \n",
    "        # If majority of frames face the camera, flip the model\n",
    "        return facing_camera_frames > facing_away_frames\n",
    "\n",
    "    def smooth_positions(self, positions, window_size=3):\n",
    "        \"\"\"Apply smoothing to reduce jitter\"\"\"\n",
    "        if len(positions) <= 1:\n",
    "            return positions\n",
    "            \n",
    "        smoothed = []\n",
    "        joint_names = positions[0].keys()\n",
    "        half_window = max(1, window_size // 2)\n",
    "        \n",
    "        for i in range(len(positions)):\n",
    "            smooth_frame = {}\n",
    "            \n",
    "            for joint in joint_names:\n",
    "                # Calculate window bounds\n",
    "                start = max(0, i - half_window)\n",
    "                end = min(len(positions), i + half_window + 1)\n",
    "                \n",
    "                # Calculate average position within window\n",
    "                x_sum = y_sum = z_sum = 0\n",
    "                count = 0\n",
    "                \n",
    "                for j in range(start, end):\n",
    "                    if joint in positions[j]:\n",
    "                        x_sum += positions[j][joint]['x']\n",
    "                        y_sum += positions[j][joint]['y']\n",
    "                        z_sum += positions[j][joint]['z']\n",
    "                        count += 1\n",
    "                \n",
    "                if count > 0:\n",
    "                    smooth_frame[joint] = {\n",
    "                        'x': x_sum / count,\n",
    "                        'y': y_sum / count,\n",
    "                        'z': z_sum / count\n",
    "                    }\n",
    "                else:\n",
    "                    smooth_frame[joint] = positions[i][joint].copy()\n",
    "            \n",
    "            smoothed.append(smooth_frame)\n",
    "        \n",
    "        return smoothed\n",
    "\n",
    "    def calculate_joint_offsets(self, reference_frame):\n",
    "        \"\"\"Calculate bone offsets from reference frame\"\"\"\n",
    "        offsets = {}\n",
    "        \n",
    "        for joint_name, children in self.joint_hierarchy.items():\n",
    "            if joint_name not in reference_frame:\n",
    "                continue\n",
    "                \n",
    "            parent_pos = reference_frame[joint_name]\n",
    "            \n",
    "            for child in children:\n",
    "                if child.startswith(\"End_\"):\n",
    "                    # Use predefined end site offset\n",
    "                    offsets[child] = self.end_site_offsets[child]\n",
    "                elif child in reference_frame:\n",
    "                    child_pos = reference_frame[child]\n",
    "                    offsets[child] = (\n",
    "                        child_pos['x'] - parent_pos['x'],\n",
    "                        child_pos['y'] - parent_pos['y'],\n",
    "                        child_pos['z'] - parent_pos['z']\n",
    "                    )\n",
    "        \n",
    "        return offsets\n",
    "\n",
    "    def find_tpose_frame(self, positions):\n",
    "        \"\"\"Try to find a T-pose frame in the sequence\"\"\"\n",
    "        best_idx = 0\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        # Check first few frames, they often contain initialization pose\n",
    "        check_frames = min(30, len(positions))\n",
    "        \n",
    "        for i in range(check_frames):\n",
    "            # Calculate T-pose score based on arm extension and symmetry\n",
    "            if \"LeftShoulder\" not in positions[i] or \"RightShoulder\" not in positions[i]:\n",
    "                continue\n",
    "                \n",
    "            left_arm_hor = 0\n",
    "            right_arm_hor = 0\n",
    "            \n",
    "            # Check horizontal arm extension\n",
    "            if \"LeftHand\" in positions[i] and \"LeftShoulder\" in positions[i]:\n",
    "                dx = positions[i][\"LeftHand\"][\"x\"] - positions[i][\"LeftShoulder\"][\"x\"]\n",
    "                dy = positions[i][\"LeftHand\"][\"y\"] - positions[i][\"LeftShoulder\"][\"y\"]\n",
    "                left_arm_hor = abs(dx) - abs(dy)  # Higher is better (more horizontal)\n",
    "                \n",
    "            if \"RightHand\" in positions[i] and \"RightShoulder\" in positions[i]:\n",
    "                dx = positions[i][\"RightHand\"][\"x\"] - positions[i][\"RightShoulder\"][\"x\"]\n",
    "                dy = positions[i][\"RightHand\"][\"y\"] - positions[i][\"RightShoulder\"][\"y\"]\n",
    "                right_arm_hor = abs(dx) - abs(dy)  # Higher is better (more horizontal)\n",
    "            \n",
    "            # Check if shoulders are level\n",
    "            shoulder_level = 0\n",
    "            if \"LeftShoulder\" in positions[i] and \"RightShoulder\" in positions[i]:\n",
    "                shoulder_level = -abs(positions[i][\"LeftShoulder\"][\"y\"] - positions[i][\"RightShoulder\"][\"y\"])\n",
    "            \n",
    "            # Calculate overall T-pose score\n",
    "            score = left_arm_hor + right_arm_hor + shoulder_level\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_idx = i\n",
    "        \n",
    "        return best_idx\n",
    "\n",
    "    def generate_direct_animation(self, positions, reference_frame, offsets):\n",
    "        \"\"\"\n",
    "        Generate animation data using direct position mode.\n",
    "        This mode prioritizes visual matching over anatomical correctness.\n",
    "        \"\"\"\n",
    "        animation_data = []\n",
    "        \n",
    "        # Get the list of all joints in hierarchical order\n",
    "        all_joints = self._get_joint_list(\"Hips\")\n",
    "        \n",
    "        for frame_idx, frame_positions in enumerate(positions):\n",
    "            frame_data = []\n",
    "            \n",
    "            # Add root position\n",
    "            root_pos = frame_positions[\"Hips\"]\n",
    "            frame_data.extend([root_pos['x'], root_pos['y'], root_pos['z']])\n",
    "            \n",
    "            # Add a global Y rotation to the hips (facing +Z direction)\n",
    "            # 180° = pi radians\n",
    "            frame_data.extend([0.0, 180.0, 0.0])\n",
    "            \n",
    "            # For each joint (except Hips which we've already handled), calculate rotations\n",
    "            for joint_name in all_joints:\n",
    "                if joint_name == \"Hips\":\n",
    "                    continue  # Root joint already handled above\n",
    "                    \n",
    "                x_rot = y_rot = z_rot = 0.0\n",
    "                \n",
    "                # Find parent of this joint\n",
    "                parent_name = self._get_parent(joint_name)\n",
    "                \n",
    "                if parent_name and parent_name in frame_positions and joint_name in frame_positions:\n",
    "                    # Get positions of this joint and its parent\n",
    "                    joint_pos = frame_positions[joint_name]\n",
    "                    parent_pos = frame_positions[parent_name]\n",
    "                    \n",
    "                    # Calculate direction vector from parent to joint\n",
    "                    dx = joint_pos['x'] - parent_pos['x']\n",
    "                    dy = joint_pos['y'] - parent_pos['y']\n",
    "                    dz = joint_pos['z'] - parent_pos['z']\n",
    "                    \n",
    "                    # Set rotations to position the joint correctly\n",
    "                    if abs(dx) > 0.001 or abs(dy) > 0.001 or abs(dz) > 0.001:\n",
    "                        # X rotation (pitch) - rotation around X axis affects Y and Z\n",
    "                        x_rot = math.degrees(math.atan2(dz, dy)) if (abs(dy) > 0.001 or abs(dz) > 0.001) else 0\n",
    "                        \n",
    "                        # Y rotation (yaw) - rotation around Y axis affects X and Z\n",
    "                        y_rot = math.degrees(math.atan2(dx, dz)) if (abs(dx) > 0.001 or abs(dz) > 0.001) else 0\n",
    "                        \n",
    "                        # Z rotation (roll) - rotation around Z axis affects X and Y\n",
    "                        z_rot = math.degrees(math.atan2(dx, dy)) if (abs(dx) > 0.001 or abs(dy) > 0.001) else 0\n",
    "                \n",
    "                # Add this joint's rotations\n",
    "                frame_data.extend([x_rot, y_rot, z_rot])\n",
    "            \n",
    "            animation_data.append(frame_data)\n",
    "        \n",
    "        return animation_data\n",
    "\n",
    "    def _get_parent(self, joint_name):\n",
    "        \"\"\"Find the parent of a joint in the hierarchy\"\"\"\n",
    "        for parent, children in self.joint_hierarchy.items():\n",
    "            if joint_name in children:\n",
    "                return parent\n",
    "        return None  # No parent found\n",
    "\n",
    "    def write_direct_bvh_file(self, positions, output_path):\n",
    "        \"\"\"Write BVH file using direct position approach for maximum visual accuracy\"\"\"\n",
    "        if len(positions) < 2:\n",
    "            raise ValueError(\"Need at least 2 frames to create animation\")\n",
    "            \n",
    "        # Find the best reference frame for skeleton\n",
    "        ref_idx = self.find_tpose_frame(positions)\n",
    "        reference_frame = positions[ref_idx]\n",
    "        print(f\"Using frame {ref_idx} as reference for skeleton\")\n",
    "        \n",
    "        # Calculate joint offsets from reference frame\n",
    "        offsets = self.calculate_joint_offsets(reference_frame)\n",
    "        \n",
    "        # Generate animation data using direct position approach\n",
    "        animation_data = self.generate_direct_animation(positions, reference_frame, offsets)\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            # Write HIERARCHY section\n",
    "            f.write(\"HIERARCHY\\n\")\n",
    "            f.write(\"ROOT Hips\\n\")\n",
    "            f.write(\"{\\n\")\n",
    "            f.write(\"\\tOFFSET 0.00 0.00 0.00\\n\")\n",
    "            f.write(\"\\tCHANNELS 6 Xposition Yposition Zposition Xrotation Yrotation Zrotation\\n\")\n",
    "            \n",
    "            # Write joint hierarchy\n",
    "            self._write_joint_hierarchy(f, \"Hips\", offsets, 1)\n",
    "            \n",
    "            # End HIERARCHY section\n",
    "            f.write(\"}\\n\")\n",
    "            \n",
    "            # Write MOTION section\n",
    "            f.write(\"MOTION\\n\")\n",
    "            f.write(f\"Frames: {len(positions)}\\n\")\n",
    "            f.write(f\"Frame Time: {1.0/self.fps:.6f}\\n\")\n",
    "            \n",
    "            # Write frame data\n",
    "            for frame_data in animation_data:\n",
    "                f.write(\" \".join(f\"{val:.6f}\" for val in frame_data) + \"\\n\")\n",
    "        \n",
    "        print(f\"BVH file written to {output_path}\")\n",
    "\n",
    "    def _write_joint_hierarchy(self, file, joint_name, offsets, indent_level):\n",
    "        \"\"\"Write joint hierarchy to BVH file\"\"\"\n",
    "        indent = \"\\t\" * indent_level\n",
    "        \n",
    "        for child in self.joint_hierarchy.get(joint_name, []):\n",
    "            if child.startswith(\"End_\"):\n",
    "                # Write end site\n",
    "                file.write(f\"{indent}End Site\\n\")\n",
    "                file.write(f\"{indent}{{\\n\")\n",
    "                \n",
    "                offset = offsets.get(child, (0, 0, 0))\n",
    "                file.write(f\"{indent}\\tOFFSET {offset[0]:.6f} {offset[1]:.6f} {offset[2]:.6f}\\n\")\n",
    "                \n",
    "                file.write(f\"{indent}}}\\n\")\n",
    "            else:\n",
    "                # Write child joint\n",
    "                file.write(f\"{indent}JOINT {child}\\n\")\n",
    "                file.write(f\"{indent}{{\\n\")\n",
    "                \n",
    "                offset = offsets.get(child, (0, 0, 0))\n",
    "                file.write(f\"{indent}\\tOFFSET {offset[0]:.6f} {offset[1]:.6f} {offset[2]:.6f}\\n\")\n",
    "                \n",
    "                # All non-root joints have rotation only\n",
    "                file.write(f\"{indent}\\tCHANNELS 3 Xrotation Yrotation Zrotation\\n\")\n",
    "                \n",
    "                # Write child's children\n",
    "                self._write_joint_hierarchy(file, child, offsets, indent_level + 1)\n",
    "                \n",
    "                file.write(f\"{indent}}}\\n\")\n",
    "\n",
    "    def _get_joint_list(self, start_joint):\n",
    "        \"\"\"Get a flat list of all joints in hierarchy order\"\"\"\n",
    "        joints = [start_joint]\n",
    "        \n",
    "        for child in self.joint_hierarchy.get(start_joint, []):\n",
    "            if not child.startswith(\"End_\"):\n",
    "                joints.extend(self._get_joint_list(child))\n",
    "        \n",
    "        return joints\n",
    "\n",
    "    def convert_video_to_bvh(self, video_path, output_path, visualize=False, max_frames=None):\n",
    "        \"\"\"Convert video to BVH file\"\"\"\n",
    "        print(f\"Processing video: {video_path}\")\n",
    "        \n",
    "        # Extract pose data from video\n",
    "        frames = self.process_video(video_path, visualize, max_frames)\n",
    "        \n",
    "        # Temporary position calculation to determine facing direction\n",
    "        temp_positions = self.calculate_joint_positions(frames, flip_forward=False)\n",
    "        should_flip_forward = self.detect_facing_direction(temp_positions)\n",
    "        \n",
    "        if should_flip_forward:\n",
    "            print(\"Detected person facing camera - will flip model 180 degrees for correct orientation\")\n",
    "        else:\n",
    "            print(\"Detected person facing away from camera - standard orientation will be used\")\n",
    "        \n",
    "        # Final position calculation with correct orientation\n",
    "        positions = self.calculate_joint_positions(frames, flip_forward=should_flip_forward)\n",
    "        \n",
    "        # Apply minimal smoothing to preserve most motion details\n",
    "        print(\"Applying minimal smoothing...\")\n",
    "        smoothed_positions = self.smooth_positions(positions, window_size=3)\n",
    "        \n",
    "        # Write BVH file using direct position approach for better visual matching\n",
    "        print(f\"Writing BVH file to: {output_path}\")\n",
    "        self.write_direct_bvh_file(smoothed_positions, output_path)\n",
    "        \n",
    "        print(\"Conversion complete!\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser(description='Convert video to BVH using MediaPipe.')\n",
    "#     parser.add_argument('--input', type=str, required=True, help='Input video file')\n",
    "#     parser.add_argument('--output', type=str, help='Output BVH file')\n",
    "#     parser.add_argument('--fps', type=int, default=30, help='Frames per second for BVH')\n",
    "#     parser.add_argument('--scale', type=float, default=100.0, help='Scale factor for the skeleton')\n",
    "#     parser.add_argument('--visualize', action='store_true', help='Save visualization of pose tracking')\n",
    "#     parser.add_argument('--max-frames', type=int, help='Maximum number of frames to process')\n",
    "#     parser.add_argument('--flip', action='store_true', help='Force 180 degree flip of the model')\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     # If output path is not specified, use input filename with .bvh extension\n",
    "#     if not args.output:\n",
    "#         base_name = os.path.splitext(os.path.basename(args.input))[0]\n",
    "#         args.output = f\"{base_name}.bvh\"\n",
    "    \n",
    "#     try:\n",
    "#         converter = MediaPipeToBVH(fps=args.fps, scale=args.scale)\n",
    "#         converter.convert_video_to_bvh(args.input, args.output, args.visualize, args.max_frames)\n",
    "#         print(f\"✓ Successfully created BVH file: {args.output}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {str(e)}\")\n",
    "#         return 1\n",
    "    \n",
    "#     return 0\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4d4cc1-f403-4419-b1b6-404045e7ee82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: dance6.mp4\n",
      "Video has 481 frames at 60.0 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740615843.576481   19896 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1740615843.578963   22964 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1740615843.652414   22944 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740615843.734535   22943 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740615843.801130   22945 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame 48/481 (10.0%)\n",
      "Processing frame 96/481 (20.0%)\n",
      "Processing frame 144/481 (29.9%)\n",
      "Processing frame 192/481 (39.9%)\n",
      "Processing frame 240/481 (49.9%)\n",
      "Processing frame 288/481 (59.9%)\n",
      "Processing frame 336/481 (69.9%)\n",
      "Processing frame 384/481 (79.8%)\n",
      "Processing frame 432/481 (89.8%)\n",
      "Processing frame 480/481 (99.8%)\n",
      "Extracted pose data from 481 frames\n",
      "Detected person facing camera - will flip model 180 degrees for correct orientation\n",
      "Applying minimal smoothing...\n",
      "Writing BVH file to: dance6.bvh\n",
      "Using frame 4 as reference for skeleton\n",
      "BVH file written to dance6.bvh\n",
      "Conversion complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"dance6\"\n",
    "converter = MediaPipeToBVH(fps=30, scale=100.0)\n",
    "converter.convert_video_to_bvh(f\"{filename}.mp4\", f\"{filename}.bvh\", visualize=True, max_frames=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df393cb7-ef2a-4137-897a-8518e99772df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
