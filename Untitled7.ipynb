{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e408032-920f-4c63-a4a6-621e6c449186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "# MediaPipe setup\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# BVH Joint Structure\n",
    "class Joint:\n",
    "    def __init__(self, name, parent=None):\n",
    "        self.name = name\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.offset = np.zeros(3)\n",
    "        self.channels = []\n",
    "        self.rotation_order = 'ZXY'  # Default rotation order\n",
    "        self.positions = []\n",
    "        self.rotations = []\n",
    "        \n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "        \n",
    "def create_skeleton():\n",
    "    \"\"\"Create a skeleton structure that matches MediaPipe's pose landmarks.\"\"\"\n",
    "    # Root joint\n",
    "    hips = Joint(\"Hips\")\n",
    "    \n",
    "    # Spine\n",
    "    spine = Joint(\"Spine\", hips)\n",
    "    hips.add_child(spine)\n",
    "    \n",
    "    chest = Joint(\"Chest\", spine)\n",
    "    spine.add_child(chest)\n",
    "    \n",
    "    neck = Joint(\"Neck\", chest)\n",
    "    chest.add_child(neck)\n",
    "    \n",
    "    head = Joint(\"Head\", neck)\n",
    "    neck.add_child(head)\n",
    "    \n",
    "    # Left arm\n",
    "    left_shoulder = Joint(\"LeftShoulder\", chest)\n",
    "    chest.add_child(left_shoulder)\n",
    "    \n",
    "    left_arm = Joint(\"LeftArm\", left_shoulder)\n",
    "    left_shoulder.add_child(left_arm)\n",
    "    \n",
    "    left_forearm = Joint(\"LeftForeArm\", left_arm)\n",
    "    left_arm.add_child(left_forearm)\n",
    "    \n",
    "    left_hand = Joint(\"LeftHand\", left_forearm)\n",
    "    left_forearm.add_child(left_hand)\n",
    "    \n",
    "    # Right arm\n",
    "    right_shoulder = Joint(\"RightShoulder\", chest)\n",
    "    chest.add_child(right_shoulder)\n",
    "    \n",
    "    right_arm = Joint(\"RightArm\", right_shoulder)\n",
    "    right_shoulder.add_child(right_arm)\n",
    "    \n",
    "    right_forearm = Joint(\"RightForeArm\", right_arm)\n",
    "    right_arm.add_child(right_forearm)\n",
    "    \n",
    "    right_hand = Joint(\"RightHand\", right_forearm)\n",
    "    right_forearm.add_child(right_hand)\n",
    "    \n",
    "    # Left leg\n",
    "    left_up_leg = Joint(\"LeftUpLeg\", hips)\n",
    "    hips.add_child(left_up_leg)\n",
    "    \n",
    "    left_leg = Joint(\"LeftLeg\", left_up_leg)\n",
    "    left_up_leg.add_child(left_leg)\n",
    "    \n",
    "    left_foot = Joint(\"LeftFoot\", left_leg)\n",
    "    left_leg.add_child(left_foot)\n",
    "    \n",
    "    left_toe = Joint(\"LeftToeBase\", left_foot)\n",
    "    left_foot.add_child(left_toe)\n",
    "    \n",
    "    # Right leg\n",
    "    right_up_leg = Joint(\"RightUpLeg\", hips)\n",
    "    hips.add_child(right_up_leg)\n",
    "    \n",
    "    right_leg = Joint(\"RightLeg\", right_up_leg)\n",
    "    right_up_leg.add_child(right_leg)\n",
    "    \n",
    "    right_foot = Joint(\"RightFoot\", right_leg)\n",
    "    right_leg.add_child(right_foot)\n",
    "    \n",
    "    right_toe = Joint(\"RightToeBase\", right_foot)\n",
    "    right_foot.add_child(right_toe)\n",
    "    \n",
    "    return hips\n",
    "\n",
    "def get_joint_mapping():\n",
    "    \"\"\"Map MediaPipe landmarks to skeleton joints.\"\"\"\n",
    "    # MediaPipe pose landmarks: https://developers.google.com/mediapipe/solutions/vision/pose_landmarker\n",
    "    return {\n",
    "        \"Hips\": [mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP],\n",
    "        \"Spine\": [mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER],\n",
    "        \"Chest\": [mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER],\n",
    "        \"Neck\": [mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.NOSE],\n",
    "        \"Head\": [mp_pose.PoseLandmark.NOSE],\n",
    "        \"LeftShoulder\": [mp_pose.PoseLandmark.LEFT_SHOULDER],\n",
    "        \"LeftArm\": [mp_pose.PoseLandmark.LEFT_ELBOW],\n",
    "        \"LeftForeArm\": [mp_pose.PoseLandmark.LEFT_WRIST],\n",
    "        \"LeftHand\": [mp_pose.PoseLandmark.LEFT_PINKY, mp_pose.PoseLandmark.LEFT_INDEX, mp_pose.PoseLandmark.LEFT_THUMB],\n",
    "        \"RightShoulder\": [mp_pose.PoseLandmark.RIGHT_SHOULDER],\n",
    "        \"RightArm\": [mp_pose.PoseLandmark.RIGHT_ELBOW],\n",
    "        \"RightForeArm\": [mp_pose.PoseLandmark.RIGHT_WRIST],\n",
    "        \"RightHand\": [mp_pose.PoseLandmark.RIGHT_PINKY, mp_pose.PoseLandmark.RIGHT_INDEX, mp_pose.PoseLandmark.RIGHT_THUMB],\n",
    "        \"LeftUpLeg\": [mp_pose.PoseLandmark.LEFT_HIP],\n",
    "        \"LeftLeg\": [mp_pose.PoseLandmark.LEFT_KNEE],\n",
    "        \"LeftFoot\": [mp_pose.PoseLandmark.LEFT_ANKLE],\n",
    "        \"LeftToeBase\": [mp_pose.PoseLandmark.LEFT_FOOT_INDEX],\n",
    "        \"RightUpLeg\": [mp_pose.PoseLandmark.RIGHT_HIP],\n",
    "        \"RightLeg\": [mp_pose.PoseLandmark.RIGHT_KNEE],\n",
    "        \"RightFoot\": [mp_pose.PoseLandmark.RIGHT_ANKLE],\n",
    "        \"RightToeBase\": [mp_pose.PoseLandmark.RIGHT_FOOT_INDEX]\n",
    "    }\n",
    "\n",
    "def calculate_joint_offsets(joint, landmarks, joint_mapping, frame_idx=0):\n",
    "    \"\"\"Calculate the offset for a joint based on MediaPipe landmarks.\"\"\"\n",
    "    if not landmarks or frame_idx >= len(landmarks):\n",
    "        print(f\"Warning: Invalid frame index {frame_idx} for joint {joint.name}\")\n",
    "        return\n",
    "        \n",
    "    if joint.name in joint_mapping:\n",
    "        indices = joint_mapping[joint.name]\n",
    "        position = np.zeros(3)\n",
    "        valid_indices = 0\n",
    "        \n",
    "        for idx in indices:\n",
    "            # Check if index is valid\n",
    "            if idx < len(landmarks[frame_idx]):\n",
    "                lm = landmarks[frame_idx][idx]\n",
    "                # Check for NaN values\n",
    "                if not (np.isnan(lm.x) or np.isnan(lm.y) or np.isnan(lm.z)):\n",
    "                    position += np.array([lm.x, lm.y, lm.z])\n",
    "                    valid_indices += 1\n",
    "        \n",
    "        if valid_indices > 0:\n",
    "            position /= valid_indices  # Average position\n",
    "        else:\n",
    "            # If no valid landmarks, use a default offset\n",
    "            position = np.array([0.0, 0.0, 0.0])\n",
    "            print(f\"Warning: No valid landmarks for joint {joint.name}\")\n",
    "        \n",
    "        if joint.parent:\n",
    "            parent_position = np.zeros(3)\n",
    "            parent_indices = joint_mapping[joint.parent.name]\n",
    "            valid_parent_indices = 0\n",
    "            \n",
    "            for idx in parent_indices:\n",
    "                if idx < len(landmarks[frame_idx]):\n",
    "                    lm = landmarks[frame_idx][idx]\n",
    "                    if not (np.isnan(lm.x) or np.isnan(lm.y) or np.isnan(lm.z)):\n",
    "                        parent_position += np.array([lm.x, lm.y, lm.z])\n",
    "                        valid_parent_indices += 1\n",
    "            \n",
    "            if valid_parent_indices > 0:\n",
    "                parent_position /= valid_parent_indices\n",
    "                \n",
    "                # Calculate offset from parent to this joint\n",
    "                offset = position - parent_position\n",
    "                \n",
    "                # Scale offset to make the skeleton more realistic\n",
    "                # MediaPipe uses a normalized coordinate system, so we scale up\n",
    "                joint.offset = offset * 100  # Scale factor can be adjusted\n",
    "            else:\n",
    "                # Default offset if parent position is invalid\n",
    "                joint.offset = np.array([0.0, 0.0, 0.1])  # Small default offset\n",
    "                print(f\"Warning: No valid parent landmarks for joint {joint.name}\")\n",
    "        else:\n",
    "            # Root joint, offset from origin\n",
    "            joint.offset = position * 100\n",
    "    \n",
    "    for child in joint.children:\n",
    "        calculate_joint_offsets(child, landmarks, joint_mapping, frame_idx)\n",
    "\n",
    "def quaternion_to_euler(q, order='ZXY'):\n",
    "    \"\"\"Convert quaternion to Euler angles based on specified order.\"\"\"\n",
    "    # q = [w, x, y, z]\n",
    "    w, x, y, z = q\n",
    "    \n",
    "    # Check for invalid quaternion\n",
    "    norm = math.sqrt(w*w + x*x + y*y + z*z)\n",
    "    if norm < 1e-10:\n",
    "        return np.zeros(3)\n",
    "        \n",
    "    # Normalize quaternion\n",
    "    w, x, y, z = w/norm, x/norm, y/norm, z/norm\n",
    "    \n",
    "    if order == 'ZXY':\n",
    "        # ZXY rotation order (common in character animation)\n",
    "        sinx = 2.0 * (w * x - y * z)\n",
    "        cosx = 1.0 - 2.0 * (x * x + y * y)\n",
    "        angle_x = math.atan2(sinx, cosx)\n",
    "        \n",
    "        siny = 2.0 * (w * y + x * z)\n",
    "        # Clamp to avoid numerical errors\n",
    "        siny = max(min(siny, 1.0), -1.0)\n",
    "        angle_y = math.asin(siny)\n",
    "        \n",
    "        sinz = 2.0 * (w * z + x * y)\n",
    "        cosz = 1.0 - 2.0 * (y * y + z * z)\n",
    "        angle_z = math.atan2(sinz, cosz)\n",
    "        \n",
    "        return np.array([math.degrees(angle_x), math.degrees(angle_y), math.degrees(angle_z)])\n",
    "    \n",
    "    else:\n",
    "        # Default to XYZ if order not recognized\n",
    "        angles = np.zeros(3)\n",
    "        # More rotation order calculations would go here\n",
    "        return angles\n",
    "\n",
    "def calculate_rotation(joint, landmarks, joint_mapping, frame_idx):\n",
    "    \"\"\"Calculate rotation for a joint based on MediaPipe landmarks.\"\"\"\n",
    "    # Safety check for invalid frame index\n",
    "    if not landmarks or frame_idx >= len(landmarks):\n",
    "        return np.zeros(3)\n",
    "        \n",
    "    if joint.name in joint_mapping and len(joint.children) > 0:\n",
    "        # Get joint position\n",
    "        indices = joint_mapping[joint.name]\n",
    "        position = np.zeros(3)\n",
    "        valid_indices = 0\n",
    "        \n",
    "        for idx in indices:\n",
    "            if idx < len(landmarks[frame_idx]):\n",
    "                lm = landmarks[frame_idx][idx]\n",
    "                if not (np.isnan(lm.x) or np.isnan(lm.y) or np.isnan(lm.z)):\n",
    "                    position += np.array([lm.x, lm.y, lm.z])\n",
    "                    valid_indices += 1\n",
    "        \n",
    "        if valid_indices > 0:\n",
    "            position /= valid_indices\n",
    "        else:\n",
    "            return np.zeros(3)  # No valid position data\n",
    "        \n",
    "        # Find the first child with valid position data\n",
    "        for child in joint.children:\n",
    "            if child.name in joint_mapping:\n",
    "                # Get child position\n",
    "                child_indices = joint_mapping[child.name]\n",
    "                child_position = np.zeros(3)\n",
    "                valid_child_indices = 0\n",
    "                \n",
    "                for idx in child_indices:\n",
    "                    if idx < len(landmarks[frame_idx]):\n",
    "                        lm = landmarks[frame_idx][idx]\n",
    "                        if not (np.isnan(lm.x) or np.isnan(lm.y) or np.isnan(lm.z)):\n",
    "                            child_position += np.array([lm.x, lm.y, lm.z])\n",
    "                            valid_child_indices += 1\n",
    "                \n",
    "                if valid_child_indices > 0:\n",
    "                    child_position /= valid_child_indices\n",
    "                    \n",
    "                    # Calculate direction vector\n",
    "                    direction = child_position - position\n",
    "                    if np.linalg.norm(direction) > 1e-10:\n",
    "                        direction = direction / np.linalg.norm(direction)\n",
    "                        \n",
    "                        # Calculate rotation to align with direction\n",
    "                        # Default forward axis is usually (0, 0, 1) in most systems\n",
    "                        forward = np.array([0, 0, 1])\n",
    "                        \n",
    "                        # Calculate rotation axis and angle\n",
    "                        axis = np.cross(forward, direction)\n",
    "                        if np.linalg.norm(axis) > 1e-10:\n",
    "                            axis = axis / np.linalg.norm(axis)\n",
    "                            \n",
    "                            dot = np.dot(forward, direction)\n",
    "                            # Clamp to avoid numerical errors\n",
    "                            dot = max(min(dot, 1.0), -1.0)\n",
    "                            angle = math.acos(dot)\n",
    "                            \n",
    "                            # Convert to quaternion\n",
    "                            sin_half = math.sin(angle/2)\n",
    "                            qw = math.cos(angle/2)\n",
    "                            qx = axis[0] * sin_half\n",
    "                            qy = axis[1] * sin_half\n",
    "                            qz = axis[2] * sin_half\n",
    "                            \n",
    "                            # Convert quaternion to Euler angles\n",
    "                            return quaternion_to_euler([qw, qx, qy, qz], joint.rotation_order)\n",
    "    \n",
    "    return np.zeros(3)  # Default rotation\n",
    "\n",
    "def process_frames(landmarks, skeleton_root, joint_mapping):\n",
    "    \"\"\"Process all frames and calculate positions and rotations for each joint.\"\"\"\n",
    "    num_frames = len(landmarks)\n",
    "    print(f\"Processing {num_frames} frames for animation data...\")\n",
    "    \n",
    "    # Initialize all joints with empty rotation arrays\n",
    "    def init_rotations(joint):\n",
    "        joint.rotations = [np.zeros(3) for _ in range(num_frames)]\n",
    "        for child in joint.children:\n",
    "            init_rotations(child)\n",
    "    \n",
    "    init_rotations(skeleton_root)\n",
    "    \n",
    "    # Process each frame\n",
    "    for frame_idx in tqdm(range(num_frames), desc=\"Calculating joint rotations\"):\n",
    "        process_frame_rotations(landmarks, skeleton_root, joint_mapping, frame_idx)\n",
    "\n",
    "def process_frame_rotations(landmarks, joint, joint_mapping, frame_idx):\n",
    "    \"\"\"Process a single frame for a joint and its children.\"\"\"\n",
    "    # Calculate rotation for this joint\n",
    "    rotation = calculate_rotation(joint, landmarks, joint_mapping, frame_idx)\n",
    "    joint.rotations[frame_idx] = rotation\n",
    "    \n",
    "    # Process children\n",
    "    for child in joint.children:\n",
    "        process_frame_rotations(landmarks, child, joint_mapping, frame_idx)\n",
    "\n",
    "def write_bvh_file(root_joint, frame_time, output_file):\n",
    "    \"\"\"Write the BVH file with motion data.\"\"\"\n",
    "    print(f\"Writing BVH file to {output_file}...\")\n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            # Write header\n",
    "            f.write(\"HIERARCHY\\n\")\n",
    "            \n",
    "            # Write joint hierarchy recursively\n",
    "            write_joint_hierarchy(f, root_joint, 0)\n",
    "            \n",
    "            # Write motion data\n",
    "            num_frames = len(root_joint.rotations)\n",
    "            f.write(\"MOTION\\n\")\n",
    "            f.write(f\"Frames: {num_frames}\\n\")\n",
    "            f.write(f\"Frame Time: {frame_time:.6f}\\n\")\n",
    "            \n",
    "            # Write frame data\n",
    "            for frame_idx in tqdm(range(num_frames), desc=\"Writing frame data\"):\n",
    "                frame_data = []\n",
    "                collect_frame_data(root_joint, frame_idx, frame_data)\n",
    "                f.write(\" \".join([f\"{val:.6f}\" for val in frame_data]) + \"\\n\")\n",
    "                \n",
    "        print(f\"BVH file created successfully: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing BVH file: {e}\")\n",
    "\n",
    "def write_joint_hierarchy(f, joint, indent_level):\n",
    "    \"\"\"Write the joint hierarchy recursively.\"\"\"\n",
    "    indent = \"  \" * indent_level\n",
    "    \n",
    "    if joint.parent is None:\n",
    "        # Root joint\n",
    "        f.write(f\"{indent}ROOT {joint.name}\\n\")\n",
    "    else:\n",
    "        # Child joint\n",
    "        f.write(f\"{indent}JOINT {joint.name}\\n\")\n",
    "    \n",
    "    f.write(f\"{indent}\" + \"{\\n\")\n",
    "    \n",
    "    # Write offset\n",
    "    f.write(f\"{indent}  OFFSET {joint.offset[0]:.6f} {joint.offset[1]:.6f} {joint.offset[2]:.6f}\\n\")\n",
    "    \n",
    "    # Write channels\n",
    "    if joint.parent is None:\n",
    "        # Root has 6 channels: position and rotation\n",
    "        f.write(f\"{indent}  CHANNELS 6 Xposition Yposition Zposition {joint.rotation_order[0]}rotation {joint.rotation_order[1]}rotation {joint.rotation_order[2]}rotation\\n\")\n",
    "        joint.channels = ['Xposition', 'Yposition', 'Zposition', f'{joint.rotation_order[0]}rotation', f'{joint.rotation_order[1]}rotation', f'{joint.rotation_order[2]}rotation']\n",
    "    else:\n",
    "        # Other joints have 3 channels: rotation only\n",
    "        f.write(f\"{indent}  CHANNELS 3 {joint.rotation_order[0]}rotation {joint.rotation_order[1]}rotation {joint.rotation_order[2]}rotation\\n\")\n",
    "        joint.channels = [f'{joint.rotation_order[0]}rotation', f'{joint.rotation_order[1]}rotation', f'{joint.rotation_order[2]}rotation']\n",
    "    \n",
    "    # Process children\n",
    "    for child in joint.children:\n",
    "        write_joint_hierarchy(f, child, indent_level + 1)\n",
    "    \n",
    "    # If no children, write end site\n",
    "    if not joint.children:\n",
    "        f.write(f\"{indent}  End Site\\n\")\n",
    "        f.write(f\"{indent}  \" + \"{\\n\")\n",
    "        f.write(f\"{indent}    OFFSET 0.0 0.0 0.0\\n\")\n",
    "        f.write(f\"{indent}  \" + \"}\\n\")\n",
    "    \n",
    "    f.write(f\"{indent}\" + \"}\\n\")\n",
    "\n",
    "def collect_frame_data(joint, frame_idx, frame_data):\n",
    "    \"\"\"Collect frame data for a specific frame.\"\"\"\n",
    "    # Add root position if this is the root joint\n",
    "    if joint.parent is None:\n",
    "        # For simplicity, we're using fixed root position here\n",
    "        # In a real implementation, you'd track the root movement from the video\n",
    "        frame_data.extend([0.0, 0.0, 0.0])  # Root position\n",
    "    \n",
    "    # Add rotation\n",
    "    if frame_idx < len(joint.rotations):\n",
    "        frame_data.extend(joint.rotations[frame_idx])\n",
    "    else:\n",
    "        # If we don't have rotation data for this frame, use zeros\n",
    "        frame_data.extend([0.0, 0.0, 0.0])\n",
    "    \n",
    "    # Process children\n",
    "    for child in joint.children:\n",
    "        collect_frame_data(child, frame_idx, frame_data)\n",
    "\n",
    "def process_video(video_path, output_bvh, confidence_threshold=0.5, sample_rate=1):\n",
    "    \"\"\"Process video and create BVH file.\"\"\"\n",
    "    print(f\"Opening video file: {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    print(f\"Video properties: {width}x{height}, {fps} FPS, {frame_count} frames\")\n",
    "    \n",
    "    # Calculate frame time based on original FPS and sampling rate\n",
    "    frame_time = 1.0 / (fps / sample_rate)\n",
    "    \n",
    "    # Create pose detector\n",
    "    print(\"Initializing MediaPipe Pose detector...\")\n",
    "    with mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,  # Use balanced model (0-fastest, 2-most accurate)\n",
    "        enable_segmentation=False,\n",
    "        min_detection_confidence=confidence_threshold\n",
    "    ) as pose:\n",
    "        \n",
    "        # Process frames\n",
    "        all_landmarks = []\n",
    "        frame_idx = 0\n",
    "        sampled_frames = 0\n",
    "        \n",
    "        print(f\"Processing video frames (sampling every {sample_rate} frames)...\")\n",
    "        with tqdm(total=frame_count) as pbar:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Process only every sample_rate frames\n",
    "                if frame_idx % sample_rate == 0:\n",
    "                    # Convert BGR to RGB\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Process the frame\n",
    "                    results = pose.process(frame_rgb)\n",
    "                    \n",
    "                    if results.pose_world_landmarks:\n",
    "                        # Store landmarks\n",
    "                        all_landmarks.append(results.pose_world_landmarks.landmark)\n",
    "                        sampled_frames += 1\n",
    "                    else:\n",
    "                        # If no landmarks detected, use the previous frame's landmarks or empty ones\n",
    "                        if all_landmarks:\n",
    "                            all_landmarks.append(all_landmarks[-1])\n",
    "                        else:\n",
    "                            # Create empty landmarks\n",
    "                            empty_landmarks = []\n",
    "                            for i in range(33):  # MediaPipe has 33 pose landmarks\n",
    "                                lm = mp.framework.formats.landmark_pb2.NormalizedLandmark()\n",
    "                                lm.x = 0\n",
    "                                lm.y = 0\n",
    "                                lm.z = 0\n",
    "                                lm.visibility = 0\n",
    "                                empty_landmarks.append(lm)\n",
    "                            all_landmarks.append(empty_landmarks)\n",
    "                        sampled_frames += 1\n",
    "                \n",
    "                frame_idx += 1\n",
    "                pbar.update(1)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        print(f\"Video processing complete. Collected {sampled_frames} frames of pose data.\")\n",
    "        \n",
    "        if not all_landmarks:\n",
    "            print(\"Error: No pose landmarks detected in the video.\")\n",
    "            return\n",
    "        \n",
    "        # Create skeleton structure\n",
    "        print(\"Creating skeleton structure...\")\n",
    "        skeleton_root = create_skeleton()\n",
    "        joint_mapping = get_joint_mapping()\n",
    "        \n",
    "        # Calculate joint offsets based on the first frame with good data\n",
    "        print(\"Calculating joint offsets...\")\n",
    "        offset_frame_idx = 0\n",
    "        max_attempts = min(10, len(all_landmarks))\n",
    "        \n",
    "        # Try to find a good frame for calculating offsets\n",
    "        for i in range(max_attempts):\n",
    "            has_valid_data = True\n",
    "            for lm in all_landmarks[i]:\n",
    "                if np.isnan(lm.x) or np.isnan(lm.y) or np.isnan(lm.z) or lm.visibility < 0.5:\n",
    "                    has_valid_data = False\n",
    "                    break\n",
    "            \n",
    "            if has_valid_data:\n",
    "                offset_frame_idx = i\n",
    "                break\n",
    "        \n",
    "        print(f\"Using frame {offset_frame_idx} for skeleton structure...\")\n",
    "        calculate_joint_offsets(skeleton_root, all_landmarks, joint_mapping, offset_frame_idx)\n",
    "        \n",
    "        # Process all frames to calculate rotations\n",
    "        process_frames(all_landmarks, skeleton_root, joint_mapping)\n",
    "        \n",
    "        # Write BVH file\n",
    "        write_bvh_file(skeleton_root, frame_time, output_bvh)\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser(description=\"Convert video to BVH using MediaPipe\")\n",
    "#     parser.add_argument(\"--video\", required=True, help=\"Path to input video file\")\n",
    "#     parser.add_argument(\"--output\", required=True, help=\"Path to output BVH file\")\n",
    "#     parser.add_argument(\"--confidence\", type=float, default=0.5, help=\"Confidence threshold for pose detection\")\n",
    "#     parser.add_argument(\"--sample-rate\", type=int, default=2, help=\"Process every Nth frame\")\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     print(\"Starting MediaPipe to BVH conversion...\")\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     process_video(args.video, args.output, args.confidence, args.sample_rate)\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     print(f\"Conversion completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "954e1f8f-dc73-405b-bef4-2431b3c5ee71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740616980.976146   24931 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening video file: fight2.mp4\n",
      "Video properties: 1280x720, 30.0 FPS, 43 frames\n",
      "Initializing MediaPipe Pose detector...\n",
      "Processing video frames (sampling every 2 frames)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740616980.976886   25667 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1740616981.048368   25654 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740616981.078778   25653 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "\n",
      "00%|███████████████████████████████████████████| 43/43 [00:00<00:00, 68.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing complete. Collected 22 frames of pose data.\n",
      "Creating skeleton structure...\n",
      "Calculating joint offsets...\n",
      "Using frame 0 for skeleton structure...\n",
      "Processing 22 frames for animation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "alculating joint rotations: 100%|█████████████| 22/22 [00:00<00:00, 601.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing BVH file to fight2.bvh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing frame data: 100%|████████████████████| 22/22 [00:00<00:00, 14600.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BVH file created successfully: fight2.bvh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"fight2\"\n",
    "process_video(f\"{filename}.mp4\", f\"{filename}.bvh\", 0.5, 2)\n",
    "# process_video(args.video, args.output, args.confidence, args.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543e772-3b97-4997-9781-1edb867bfe6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
