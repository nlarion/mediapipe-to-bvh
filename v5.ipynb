{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87e91e6-0760-4e3e-99c0-e1c96cc38de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 16:51:28.343813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740617488.362117   26700 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740617488.368392   26700 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 16:51:28.386993: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import argparse\n",
    "from typing import List, Dict, Tuple\n",
    "import os\n",
    "\n",
    "class MediaPipeToBVH:\n",
    "    def __init__(self, fps: int = 30):\n",
    "        \"\"\"\n",
    "        Initialize the MediaPipe to BVH converter.\n",
    "        \n",
    "        Args:\n",
    "            fps: Frames per second for the output BVH file\n",
    "        \"\"\"\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.fps = fps\n",
    "        \n",
    "        # Define the BVH skeleton hierarchy\n",
    "        self.joint_hierarchy = {\n",
    "            \"Hips\": [\"Spine\", \"LeftUpLeg\", \"RightUpLeg\"],\n",
    "            \"Spine\": [\"Spine1\"],\n",
    "            \"Spine1\": [\"Spine2\"],\n",
    "            \"Spine2\": [\"Neck\", \"LeftShoulder\", \"RightShoulder\"],\n",
    "            \"Neck\": [\"Head\"],\n",
    "            \"Head\": [],\n",
    "            \"LeftShoulder\": [\"LeftArm\"],\n",
    "            \"LeftArm\": [\"LeftForeArm\"],\n",
    "            \"LeftForeArm\": [\"LeftHand\"],\n",
    "            \"LeftHand\": [],\n",
    "            \"RightShoulder\": [\"RightArm\"],\n",
    "            \"RightArm\": [\"RightForeArm\"],\n",
    "            \"RightForeArm\": [\"RightHand\"],\n",
    "            \"RightHand\": [],\n",
    "            \"LeftUpLeg\": [\"LeftLeg\"],\n",
    "            \"LeftLeg\": [\"LeftFoot\"],\n",
    "            \"LeftFoot\": [\"LeftToe\"],\n",
    "            \"LeftToe\": [],\n",
    "            \"RightUpLeg\": [\"RightLeg\"],\n",
    "            \"RightLeg\": [\"RightFoot\"],\n",
    "            \"RightFoot\": [\"RightToe\"],\n",
    "            \"RightToe\": []\n",
    "        }\n",
    "        \n",
    "        # MediaPipe landmark indices mapping to BVH joints\n",
    "        self.landmark_to_joint = {\n",
    "            \"Hips\": [23, 24],  # Mid-point of left and right hip\n",
    "            \"Spine\": [11],\n",
    "            \"Spine1\": [12],\n",
    "            \"Spine2\": [12],  # MediaPipe doesn't have enough spine points\n",
    "            \"Neck\": [11],  # Approximation\n",
    "            \"Head\": [0],\n",
    "            \"LeftShoulder\": [11, 13],  # Using multiple landmarks for better positioning\n",
    "            \"LeftArm\": [13],\n",
    "            \"LeftForeArm\": [15],\n",
    "            \"LeftHand\": [19],\n",
    "            \"RightShoulder\": [12, 14],\n",
    "            \"RightArm\": [14],\n",
    "            \"RightForeArm\": [16],\n",
    "            \"RightHand\": [20],\n",
    "            \"LeftUpLeg\": [23],\n",
    "            \"LeftLeg\": [25],\n",
    "            \"LeftFoot\": [27],\n",
    "            \"LeftToe\": [31],\n",
    "            \"RightUpLeg\": [24],\n",
    "            \"RightLeg\": [26],\n",
    "            \"RightFoot\": [28],\n",
    "            \"RightToe\": [32]\n",
    "        }\n",
    "        \n",
    "        # Initialize joint offset data\n",
    "        self.joint_offsets = {}\n",
    "        \n",
    "        # Initialize pose tracking\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            enable_segmentation=False,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "    def process_video(self, video_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process video and extract pose landmarks.\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to the video file\n",
    "            \n",
    "        Returns:\n",
    "            List of pose landmarks for each frame\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file {video_path}\")\n",
    "        \n",
    "        frames = []\n",
    "        success = True\n",
    "        \n",
    "        while success:\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            \n",
    "            # Convert the BGR image to RGB\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process the image and get pose landmarks\n",
    "            results = self.pose.process(image)\n",
    "            \n",
    "            if results.pose_world_landmarks:\n",
    "                # Convert landmarks to a dictionary format\n",
    "                landmarks_dict = {}\n",
    "                for i, landmark in enumerate(results.pose_world_landmarks.landmark):\n",
    "                    landmarks_dict[i] = {\n",
    "                        'x': landmark.x,\n",
    "                        'y': landmark.y,\n",
    "                        'z': landmark.z,\n",
    "                        'visibility': landmark.visibility\n",
    "                    }\n",
    "                frames.append(landmarks_dict)\n",
    "            else:\n",
    "                # If no pose detected, duplicate the last frame or use a default pose\n",
    "                if frames:\n",
    "                    frames.append(frames[-1])\n",
    "                else:\n",
    "                    print(f\"No pose detected in initial frame. Check your video.\")\n",
    "                    return []\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def calculate_joint_positions(self, frames: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate BVH joint positions from MediaPipe landmarks.\n",
    "        \n",
    "        Args:\n",
    "            frames: List of pose landmarks for each frame\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of joint positions for each frame\n",
    "        \"\"\"\n",
    "        joint_positions = []\n",
    "        \n",
    "        for frame_idx, landmarks in enumerate(frames):\n",
    "            frame_positions = {}\n",
    "            \n",
    "            # Calculate position for each joint\n",
    "            for joint_name, landmark_indices in self.landmark_to_joint.items():\n",
    "                if len(landmark_indices) == 1:\n",
    "                    # Single landmark\n",
    "                    lm_idx = landmark_indices[0]\n",
    "                    frame_positions[joint_name] = {\n",
    "                        'x': landmarks[lm_idx]['x'],\n",
    "                        'y': landmarks[lm_idx]['y'],\n",
    "                        'z': landmarks[lm_idx]['z']\n",
    "                    }\n",
    "                else:\n",
    "                    # Average of multiple landmarks\n",
    "                    x = sum(landmarks[idx]['x'] for idx in landmark_indices) / len(landmark_indices)\n",
    "                    y = sum(landmarks[idx]['y'] for idx in landmark_indices) / len(landmark_indices)\n",
    "                    z = sum(landmarks[idx]['z'] for idx in landmark_indices) / len(landmark_indices)\n",
    "                    frame_positions[joint_name] = {'x': x, 'y': y, 'z': z}\n",
    "            \n",
    "            joint_positions.append(frame_positions)\n",
    "        \n",
    "        # Calculate joint offsets from the first frame (T-pose preferred)\n",
    "        self.calculate_joint_offsets(joint_positions[0])\n",
    "        \n",
    "        return joint_positions\n",
    "\n",
    "    def calculate_joint_offsets(self, first_frame: Dict):\n",
    "        \"\"\"\n",
    "        Calculate joint offsets based on the first frame.\n",
    "        \n",
    "        Args:\n",
    "            first_frame: Joint positions in the first frame\n",
    "        \"\"\"\n",
    "        for joint_name, children in self.joint_hierarchy.items():\n",
    "            parent_pos = first_frame[joint_name]\n",
    "            \n",
    "            for child in children:\n",
    "                child_pos = first_frame[child]\n",
    "                \n",
    "                # Calculate offset from parent to child\n",
    "                offset_x = child_pos['x'] - parent_pos['x']\n",
    "                offset_y = child_pos['y'] - parent_pos['y']\n",
    "                offset_z = child_pos['z'] - parent_pos['z']\n",
    "                \n",
    "                self.joint_offsets[child] = (offset_x, offset_y, offset_z)\n",
    "        \n",
    "        # Set Hips offset to 0\n",
    "        self.joint_offsets[\"Hips\"] = (0, 0, 0)\n",
    "\n",
    "    def calculate_joint_rotations(self, joint_positions: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate joint rotations from positions.\n",
    "        \n",
    "        Args:\n",
    "            joint_positions: Joint positions for each frame\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of joint rotations for each frame\n",
    "        \"\"\"\n",
    "        joint_rotations = []\n",
    "        \n",
    "        for frame_idx, frame_positions in enumerate(joint_positions):\n",
    "            frame_rotations = {}\n",
    "            \n",
    "            # Calculate rotation for each joint with children\n",
    "            for joint_name, children in self.joint_hierarchy.items():\n",
    "                if not children:\n",
    "                    # End sites don't have rotations\n",
    "                    frame_rotations[joint_name] = (0, 0, 0)\n",
    "                    continue\n",
    "                \n",
    "                parent_pos = frame_positions[joint_name]\n",
    "                \n",
    "                # Calculate average direction vector from parent to children\n",
    "                dir_vectors = []\n",
    "                for child in children:\n",
    "                    child_pos = frame_positions[child]\n",
    "                    dir_x = child_pos['x'] - parent_pos['x']\n",
    "                    dir_y = child_pos['y'] - parent_pos['y']\n",
    "                    dir_z = child_pos['z'] - parent_pos['z']\n",
    "                    mag = math.sqrt(dir_x**2 + dir_y**2 + dir_z**2)\n",
    "                    if mag > 0:\n",
    "                        dir_vectors.append((dir_x/mag, dir_y/mag, dir_z/mag))\n",
    "                \n",
    "                if dir_vectors:\n",
    "                    # Average direction vectors\n",
    "                    avg_dir = [sum(v[i] for v in dir_vectors)/len(dir_vectors) for i in range(3)]\n",
    "                    \n",
    "                    # Convert direction to Euler angles (ZXY order)\n",
    "                    # This is a simplified conversion and might need refinement\n",
    "                    x_rot = math.atan2(-avg_dir[2], math.sqrt(avg_dir[0]**2 + avg_dir[1]**2))\n",
    "                    y_rot = math.atan2(avg_dir[0], avg_dir[1])\n",
    "                    z_rot = 0  # Simplified assumption\n",
    "                    \n",
    "                    frame_rotations[joint_name] = (\n",
    "                        math.degrees(x_rot),\n",
    "                        math.degrees(y_rot),\n",
    "                        math.degrees(z_rot)\n",
    "                    )\n",
    "                else:\n",
    "                    frame_rotations[joint_name] = (0, 0, 0)\n",
    "            \n",
    "            joint_rotations.append(frame_rotations)\n",
    "        \n",
    "        return joint_rotations\n",
    "\n",
    "    def write_bvh_file(self, output_path: str, joint_positions: List[Dict], joint_rotations: List[Dict]):\n",
    "        \"\"\"\n",
    "        Write BVH file from joint positions and rotations.\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to save the BVH file\n",
    "            joint_positions: Joint positions for each frame\n",
    "            joint_rotations: Joint rotations for each frame\n",
    "        \"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            # Write header\n",
    "            f.write(\"HIERARCHY\\n\")\n",
    "            \n",
    "            # Write root joint (Hips)\n",
    "            f.write(\"ROOT Hips\\n\")\n",
    "            f.write(\"{\\n\")\n",
    "            \n",
    "            # Write Hips offset\n",
    "            f.write(f\"\\tOFFSET {0:.6f} {0:.6f} {0:.6f}\\n\")\n",
    "            \n",
    "            # Write Hips channels\n",
    "            f.write(\"\\tCHANNELS 6 Xposition Yposition Zposition Zrotation Xrotation Yrotation\\n\")\n",
    "            \n",
    "            # Write child joints recursively\n",
    "            self._write_joint_hierarchy(f, \"Hips\", 1)\n",
    "            \n",
    "            # End of hierarchy\n",
    "            f.write(\"}\\n\")\n",
    "            \n",
    "            # Write motion data\n",
    "            f.write(\"MOTION\\n\")\n",
    "            f.write(f\"Frames: {len(joint_positions)}\\n\")\n",
    "            f.write(f\"Frame Time: {1.0/self.fps:.6f}\\n\")\n",
    "            \n",
    "            # Write frame data\n",
    "            for frame_idx in range(len(joint_positions)):\n",
    "                frame_data = []\n",
    "                \n",
    "                # Add root position\n",
    "                hips_pos = joint_positions[frame_idx][\"Hips\"]\n",
    "                frame_data.extend([hips_pos['x'], hips_pos['y'], hips_pos['z']])\n",
    "                \n",
    "                # Add all joint rotations in order\n",
    "                self._add_joint_rotations(frame_data, joint_rotations[frame_idx], \"Hips\")\n",
    "                \n",
    "                # Write frame data line\n",
    "                f.write(\" \".join(f\"{val:.6f}\" for val in frame_data) + \"\\n\")\n",
    "\n",
    "    def _write_joint_hierarchy(self, file, joint_name: str, indent_level: int):\n",
    "        \"\"\"\n",
    "        Recursively write joint hierarchy to BVH file.\n",
    "        \n",
    "        Args:\n",
    "            file: File object to write to\n",
    "            joint_name: Current joint name\n",
    "            indent_level: Current indentation level\n",
    "        \"\"\"\n",
    "        indent = \"\\t\" * indent_level\n",
    "        \n",
    "        # Write children\n",
    "        for child in self.joint_hierarchy[joint_name]:\n",
    "            offset = self.joint_offsets[child]\n",
    "            \n",
    "            file.write(f\"{indent}JOINT {child}\\n\")\n",
    "            file.write(f\"{indent}{{\\n\")\n",
    "            file.write(f\"{indent}\\tOFFSET {offset[0]:.6f} {offset[1]:.6f} {offset[2]:.6f}\\n\")\n",
    "            \n",
    "            # Write channels\n",
    "            file.write(f\"{indent}\\tCHANNELS 3 Zrotation Xrotation Yrotation\\n\")\n",
    "            \n",
    "            # Recursively write child's children\n",
    "            if self.joint_hierarchy[child]:\n",
    "                self._write_joint_hierarchy(file, child, indent_level + 1)\n",
    "            else:\n",
    "                # End site for leaf joints\n",
    "                file.write(f\"{indent}\\tEnd Site\\n\")\n",
    "                file.write(f\"{indent}\\t{{\\n\")\n",
    "                file.write(f\"{indent}\\t\\tOFFSET 0.00 0.00 0.00\\n\")\n",
    "                file.write(f\"{indent}\\t}}\\n\")\n",
    "            \n",
    "            file.write(f\"{indent}}}\\n\")\n",
    "\n",
    "    def _add_joint_rotations(self, frame_data: List[float], rotations: Dict, joint_name: str):\n",
    "        \"\"\"\n",
    "        Recursively add joint rotations to frame data in the correct order.\n",
    "        \n",
    "        Args:\n",
    "            frame_data: List to append rotation data to\n",
    "            rotations: Dictionary of rotations for the current frame\n",
    "            joint_name: Current joint name\n",
    "        \"\"\"\n",
    "        # Add current joint rotation\n",
    "        rot = rotations[joint_name]\n",
    "        frame_data.extend([rot[2], rot[0], rot[1]])  # ZXY order\n",
    "        \n",
    "        # Add children rotations in order\n",
    "        for child in self.joint_hierarchy[joint_name]:\n",
    "            self._add_joint_rotations(frame_data, rotations, child)\n",
    "\n",
    "    def convert_video_to_bvh(self, video_path: str, output_path: str):\n",
    "        \"\"\"\n",
    "        Convert video to BVH file.\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to input video file\n",
    "            output_path: Path to save output BVH file\n",
    "        \"\"\"\n",
    "        print(f\"Processing video: {video_path}\")\n",
    "        frames = self.process_video(video_path)\n",
    "        \n",
    "        if not frames:\n",
    "            print(\"No pose data extracted. Check your video.\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"Extracted pose data from {len(frames)} frames\")\n",
    "        \n",
    "        joint_positions = self.calculate_joint_positions(frames)\n",
    "        joint_rotations = self.calculate_joint_rotations(joint_positions)\n",
    "        \n",
    "        print(f\"Writing BVH file to: {output_path}\")\n",
    "        self.write_bvh_file(output_path, joint_positions, joint_rotations)\n",
    "        \n",
    "        print(\"Conversion complete\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser(description='Convert video to BVH using MediaPipe.')\n",
    "#     parser.add_argument('--input', type=str, required=True, help='Input video file')\n",
    "#     parser.add_argument('--output', type=str, help='Output BVH file')\n",
    "#     parser.add_argument('--fps', type=int, default=30, help='Frames per second for BVH')\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     # If output is not specified, use input filename with .bvh extension\n",
    "#     if not args.output:\n",
    "#         base_name = os.path.splitext(os.path.basename(args.input))[0]\n",
    "#         args.output = f\"{base_name}.bvh\"\n",
    "    \n",
    "#     converter = MediaPipeToBVH(fps=args.fps)\n",
    "#     converter.convert_video_to_bvh(args.input, args.output)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028ada67-069b-4f66-9983-990897220eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740617492.163329   26700 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1740617492.166407   26792 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1740617492.233940   26772 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740617492.315656   26771 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: fight2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740617492.387900   26771 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pose data from 43 frames\n",
      "Writing BVH file to: test.bvh\n",
      "Conversion complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"fight2\"\n",
    "\n",
    "converter = MediaPipeToBVH(fps=30)\n",
    "converter.convert_video_to_bvh(f\"{filename}.mp4\", \"test.bvh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e408c1d-6fd0-4d8a-a8a8-bb71e5d2505c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
