{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28fb25d3-2f88-409f-af22-93c3c3522ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 16:49:25.804750: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740617365.820725   26298 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740617365.825852   26298 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 16:49:25.842144: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# BVH creation utilities\n",
    "class Joint:\n",
    "    def __init__(self, name, parent=None):\n",
    "        self.name = name\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.offset = np.zeros(3)\n",
    "        self.channels = []\n",
    "        self.motion = []\n",
    "        \n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "        \n",
    "class BVHSkeleton:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        self.joints = {}\n",
    "        self.frames = 0\n",
    "        self.frame_time = 1.0/30.0  # Default to 30fps\n",
    "        \n",
    "    def create_hierarchy(self, landmark_names):\n",
    "        # Create a simplified hierarchy for MediaPipe landmarks\n",
    "        # Root joint (typically hips in BVH)\n",
    "        self.root = Joint(\"Hips\")\n",
    "        self.joints[\"Hips\"] = self.root\n",
    "        \n",
    "        # Setup spine chain\n",
    "        spine = Joint(\"Spine\", self.root)\n",
    "        self.root.add_child(spine)\n",
    "        self.joints[\"Spine\"] = spine\n",
    "        \n",
    "        neck = Joint(\"Neck\", spine)\n",
    "        spine.add_child(neck)\n",
    "        self.joints[\"Neck\"] = neck\n",
    "        \n",
    "        head = Joint(\"Head\", neck)\n",
    "        neck.add_child(head)\n",
    "        self.joints[\"Head\"] = head\n",
    "        \n",
    "        # Left arm\n",
    "        left_shoulder = Joint(\"LeftShoulder\", spine)\n",
    "        spine.add_child(left_shoulder)\n",
    "        self.joints[\"LeftShoulder\"] = left_shoulder\n",
    "        \n",
    "        left_arm = Joint(\"LeftArm\", left_shoulder)\n",
    "        left_shoulder.add_child(left_arm)\n",
    "        self.joints[\"LeftArm\"] = left_arm\n",
    "        \n",
    "        left_forearm = Joint(\"LeftForeArm\", left_arm)\n",
    "        left_arm.add_child(left_forearm)\n",
    "        self.joints[\"LeftForeArm\"] = left_forearm\n",
    "        \n",
    "        left_hand = Joint(\"LeftHand\", left_forearm)\n",
    "        left_forearm.add_child(left_hand)\n",
    "        self.joints[\"LeftHand\"] = left_hand\n",
    "        \n",
    "        # Right arm\n",
    "        right_shoulder = Joint(\"RightShoulder\", spine)\n",
    "        spine.add_child(right_shoulder)\n",
    "        self.joints[\"RightShoulder\"] = right_shoulder\n",
    "        \n",
    "        right_arm = Joint(\"RightArm\", right_shoulder)\n",
    "        right_shoulder.add_child(right_arm)\n",
    "        self.joints[\"RightArm\"] = right_arm\n",
    "        \n",
    "        right_forearm = Joint(\"RightForeArm\", right_arm)\n",
    "        right_arm.add_child(right_forearm)\n",
    "        self.joints[\"RightForeArm\"] = right_forearm\n",
    "        \n",
    "        right_hand = Joint(\"RightHand\", right_forearm)\n",
    "        right_forearm.add_child(right_hand)\n",
    "        self.joints[\"RightHand\"] = right_hand\n",
    "        \n",
    "        # Left leg\n",
    "        left_upleg = Joint(\"LeftUpLeg\", self.root)\n",
    "        self.root.add_child(left_upleg)\n",
    "        self.joints[\"LeftUpLeg\"] = left_upleg\n",
    "        \n",
    "        left_leg = Joint(\"LeftLeg\", left_upleg)\n",
    "        left_upleg.add_child(left_leg)\n",
    "        self.joints[\"LeftLeg\"] = left_leg\n",
    "        \n",
    "        left_foot = Joint(\"LeftFoot\", left_leg)\n",
    "        left_leg.add_child(left_foot)\n",
    "        self.joints[\"LeftFoot\"] = left_foot\n",
    "        \n",
    "        # Right leg\n",
    "        right_upleg = Joint(\"RightUpLeg\", self.root)\n",
    "        self.root.add_child(right_upleg)\n",
    "        self.joints[\"RightUpLeg\"] = right_upleg\n",
    "        \n",
    "        right_leg = Joint(\"RightLeg\", right_upleg)\n",
    "        right_upleg.add_child(right_leg)\n",
    "        self.joints[\"RightLeg\"] = right_leg\n",
    "        \n",
    "        right_foot = Joint(\"RightFoot\", right_leg)\n",
    "        right_leg.add_child(right_foot)\n",
    "        self.joints[\"RightFoot\"] = right_foot\n",
    "        \n",
    "        # Setup channels for each joint\n",
    "        self.setup_channels()\n",
    "    \n",
    "    def setup_channels(self):\n",
    "        # Root has 6 channels: position and rotation\n",
    "        self.root.channels = [\"Xposition\", \"Yposition\", \"Zposition\", \"Zrotation\", \"Xrotation\", \"Yrotation\"]\n",
    "        \n",
    "        # All other joints have 3 channels (rotation only)\n",
    "        for name, joint in self.joints.items():\n",
    "            if joint != self.root:\n",
    "                joint.channels = [\"Zrotation\", \"Xrotation\", \"Yrotation\"]\n",
    "    \n",
    "    def update_joint_offsets(self, landmarks):\n",
    "        # Use first frame landmarks to set initial offsets between joints\n",
    "        scale_factor = 100  # Scale factor to make the skeleton dimensions appropriate for BVH\n",
    "        \n",
    "        # Map MediaPipe landmarks to BVH joints - these are approximate mappings\n",
    "        mapping = {\n",
    "            \"Hips\": 23,  # hip center\n",
    "            \"Spine\": 11,  # spine mid-point\n",
    "            \"Neck\": 12,   # shoulder center\n",
    "            \"Head\": 0,    # nose\n",
    "            \"LeftShoulder\": 11,  # left shoulder\n",
    "            \"LeftArm\": 13,       # left elbow\n",
    "            \"LeftForeArm\": 15,   # left wrist\n",
    "            \"LeftHand\": 19,      # left index finger\n",
    "            \"RightShoulder\": 12, # right shoulder\n",
    "            \"RightArm\": 14,      # right elbow\n",
    "            \"RightForeArm\": 16,  # right wrist\n",
    "            \"RightHand\": 20,     # right index finger\n",
    "            \"LeftUpLeg\": 23,     # left hip\n",
    "            \"LeftLeg\": 25,       # left knee\n",
    "            \"LeftFoot\": 27,      # left ankle\n",
    "            \"RightUpLeg\": 24,    # right hip\n",
    "            \"RightLeg\": 26,      # right knee\n",
    "            \"RightFoot\": 28      # right ankle\n",
    "        }\n",
    "        \n",
    "        for name, joint in self.joints.items():\n",
    "            if joint.parent is None:  # Root joint\n",
    "                joint.offset = np.zeros(3)\n",
    "            else:\n",
    "                # Calculate offset from parent\n",
    "                parent_idx = mapping[joint.parent.name]\n",
    "                child_idx = mapping[name]\n",
    "                \n",
    "                parent_pos = np.array([landmarks[parent_idx].x, landmarks[parent_idx].y, landmarks[parent_idx].z])\n",
    "                child_pos = np.array([landmarks[child_idx].x, landmarks[child_idx].y, landmarks[child_idx].z])\n",
    "                \n",
    "                # Calculate offset and apply scale\n",
    "                offset = (child_pos - parent_pos) * scale_factor\n",
    "                \n",
    "                # In BVH, Y is up, but in MediaPipe, Y is down\n",
    "                joint.offset = np.array([offset[0], -offset[1], offset[2]])\n",
    "    \n",
    "    def calculate_rotation(self, joint_name, parent_idx, child_idx, landmarks):\n",
    "        # This is a simplified rotation calculation based on direction vectors\n",
    "        # In a real implementation, you would use more sophisticated quaternion methods\n",
    "        \n",
    "        if parent_idx is None or child_idx is None:\n",
    "            return [0, 0, 0]\n",
    "            \n",
    "        # Get parent and child positions\n",
    "        parent_pos = np.array([landmarks[parent_idx].x, landmarks[parent_idx].y, landmarks[parent_idx].z])\n",
    "        child_pos = np.array([landmarks[child_idx].x, landmarks[child_idx].y, landmarks[child_idx].z])\n",
    "        \n",
    "        # Calculate direction vector\n",
    "        direction = child_pos - parent_pos\n",
    "        \n",
    "        # Normalize\n",
    "        length = np.linalg.norm(direction)\n",
    "        if length > 0:\n",
    "            direction = direction / length\n",
    "        \n",
    "        # Simple rotation calculation (this is a simplification)\n",
    "        # A better approach would use forward kinematics and quaternions\n",
    "        x_rot = np.arctan2(direction[2], direction[1]) * 180 / np.pi\n",
    "        y_rot = np.arctan2(direction[0], direction[2]) * 180 / np.pi\n",
    "        z_rot = np.arctan2(direction[1], direction[0]) * 180 / np.pi\n",
    "        \n",
    "        return [z_rot, x_rot, y_rot]\n",
    "    \n",
    "    def process_frame(self, landmarks):\n",
    "        # Process landmarks for a single frame\n",
    "        frame_data = []\n",
    "        \n",
    "        # Root position (global translation)\n",
    "        hips_idx = 23  # MediaPipe hip center index\n",
    "        root_pos = [\n",
    "            landmarks[hips_idx].x * 100,\n",
    "            -landmarks[hips_idx].y * 100,\n",
    "            landmarks[hips_idx].z * 100\n",
    "        ]\n",
    "        \n",
    "        # Add root position\n",
    "        frame_data.extend(root_pos)\n",
    "        \n",
    "        # Map landmarks to joints for rotation calculation\n",
    "        mapping = {\n",
    "            \"Hips\": (None, 23),  # hip center\n",
    "            \"Spine\": (23, 11),   # hip to spine\n",
    "            \"Neck\": (11, 12),    # spine to neck\n",
    "            \"Head\": (12, 0),     # neck to nose\n",
    "            \"LeftShoulder\": (11, 13),  # spine to left shoulder\n",
    "            \"LeftArm\": (13, 15),       # left shoulder to left elbow\n",
    "            \"LeftForeArm\": (15, 19),   # left elbow to left wrist\n",
    "            \"LeftHand\": (19, 21),      # left wrist to left index finger\n",
    "            \"RightShoulder\": (12, 14), # spine to right shoulder\n",
    "            \"RightArm\": (14, 16),      # right shoulder to right elbow\n",
    "            \"RightForeArm\": (16, 20),  # right elbow to right wrist\n",
    "            \"RightHand\": (20, 22),     # right wrist to right index finger\n",
    "            \"LeftUpLeg\": (23, 25),     # left hip to left knee\n",
    "            \"LeftLeg\": (25, 27),       # left knee to left ankle\n",
    "            \"LeftFoot\": (27, 31),      # left ankle to left foot\n",
    "            \"RightUpLeg\": (24, 26),    # right hip to right knee\n",
    "            \"RightLeg\": (26, 28),      # right knee to right ankle\n",
    "            \"RightFoot\": (28, 32)      # right ankle to right foot\n",
    "        }\n",
    "        \n",
    "        # Calculate joint rotations for each joint\n",
    "        for name, joint in self.joints.items():\n",
    "            parent_idx, child_idx = mapping.get(name, (None, None))\n",
    "            \n",
    "            if joint == self.root:\n",
    "                # Root joint: add position (already added) and rotation\n",
    "                rotation = self.calculate_rotation(name, parent_idx, child_idx, landmarks)\n",
    "                frame_data.extend(rotation)\n",
    "            else:\n",
    "                # Other joints: add rotation only\n",
    "                rotation = self.calculate_rotation(name, parent_idx, child_idx, landmarks)\n",
    "                frame_data.extend(rotation)\n",
    "        \n",
    "        return frame_data\n",
    "    \n",
    "    def write_bvh(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            # Write header\n",
    "            f.write(\"HIERARCHY\\n\")\n",
    "            \n",
    "            # Write joints recursively\n",
    "            self._write_joint(f, self.root, 0)\n",
    "            \n",
    "            # Write motion data\n",
    "            f.write(\"MOTION\\n\")\n",
    "            f.write(f\"Frames: {self.frames}\\n\")\n",
    "            f.write(f\"Frame Time: {self.frame_time}\\n\")\n",
    "            \n",
    "            # Write frame data\n",
    "            for frame in range(self.frames):\n",
    "                line = []\n",
    "                for joint_name, joint in self.joints.items():\n",
    "                    channels = len(joint.channels)\n",
    "                    \n",
    "                    if not joint.motion or frame >= len(joint.motion):\n",
    "                        # If no motion data, write zeros\n",
    "                        line.extend([0.0] * channels)\n",
    "                    else:\n",
    "                        channel_data = joint.motion[frame]\n",
    "                        if len(channel_data) < channels:\n",
    "                            # Ensure correct number of channels\n",
    "                            channel_data = channel_data + [0.0] * (channels - len(channel_data))\n",
    "                        line.extend(channel_data)\n",
    "                \n",
    "                f.write(\" \".join(map(lambda x: f\"{x:.6f}\", line)) + \"\\n\")\n",
    "    \n",
    "    def _write_joint(self, f, joint, indent_level):\n",
    "        indent = \"  \" * indent_level\n",
    "        \n",
    "        if joint.parent is None:\n",
    "            # Root joint\n",
    "            f.write(f\"{indent}ROOT {joint.name}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{indent}JOINT {joint.name}\\n\")\n",
    "        \n",
    "        f.write(f\"{indent}{{\\n\")\n",
    "        \n",
    "        # Write offset\n",
    "        offset_str = \" \".join(map(lambda x: f\"{x:.6f}\", joint.offset))\n",
    "        f.write(f\"{indent}  OFFSET {offset_str}\\n\")\n",
    "        \n",
    "        # Write channels\n",
    "        channels_str = \" \".join(joint.channels)\n",
    "        f.write(f\"{indent}  CHANNELS {len(joint.channels)} {channels_str}\\n\")\n",
    "        \n",
    "        # Write children\n",
    "        for child in joint.children:\n",
    "            self._write_joint(f, child, indent_level + 1)\n",
    "        \n",
    "        f.write(f\"{indent}}}\\n\")\n",
    "\n",
    "def process_video(input_file, output_file, visualize=False, debug=True):\n",
    "    # Initialize MediaPipe Pose\n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    \n",
    "    # Create BVH skeleton\n",
    "    skeleton = BVHSkeleton()\n",
    "    \n",
    "    # Verify file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Error: Input file '{input_file}' does not exist.\")\n",
    "        return False\n",
    "    \n",
    "    # Open video file\n",
    "    print(f\"Opening video file: {input_file}\")\n",
    "    cap = cv2.VideoCapture(input_file)\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file '{input_file}'.\")\n",
    "        print(\"Please check that the file exists and is a valid video format.\")\n",
    "        print(\"Supported formats include: mp4, avi, mov, etc.\")\n",
    "        return False\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps <= 0:\n",
    "        fps = 30  # Default if unable to determine\n",
    "        print(f\"Warning: Could not determine video FPS, using default of {fps}\")\n",
    "    \n",
    "    skeleton.frame_time = 1.0 / fps\n",
    "    \n",
    "    # Count frames\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    print(f\"Video info: {width}x{height}, {fps} FPS, {total_frames} frames\")\n",
    "    \n",
    "    if total_frames <= 0:\n",
    "        print(\"Warning: Could not determine total frame count\")\n",
    "        total_frames = float('inf')  # Process until end of video\n",
    "    \n",
    "    # Initialize variables\n",
    "    frame_count = 0\n",
    "    processed_count = 0\n",
    "    is_first_frame = True\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Setup Pose detection\n",
    "    print(\"Initializing MediaPipe pose detection...\")\n",
    "    \n",
    "    pose_config = {\n",
    "        'min_detection_confidence': 0.5,\n",
    "        'min_tracking_confidence': 0.5,\n",
    "        'model_complexity': 1  # 0=Lite, 1=Full, 2=Heavy (more accurate but slower)\n",
    "    }\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"MediaPipe pose config: {pose_config}\")\n",
    "    \n",
    "    with mp_pose.Pose(**pose_config) as pose:\n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                if debug and frame_count < total_frames:\n",
    "                    print(f\"Warning: Failed to read frame {frame_count}. Stopping.\")\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            if frame_count % 100 == 0 or debug:\n",
    "                print(f\"Processing frame {frame_count}/{total_frames if total_frames != float('inf') else '?'}\")\n",
    "            \n",
    "            # Convert the BGR image to RGB\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # For better performance, optionally mark the image as not writeable\n",
    "            image_rgb.flags.writeable = False\n",
    "            \n",
    "            # Process the image and detect pose\n",
    "            try:\n",
    "                results = pose.process(image_rgb)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {frame_count}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Enable image writing for visualization\n",
    "            image_rgb.flags.writeable = True\n",
    "            \n",
    "            # Check if pose landmarks are detected\n",
    "            if results.pose_landmarks:\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Debug landmark visibility\n",
    "                if debug and processed_count == 1:\n",
    "                    for i, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "                        print(f\"Landmark {i}: vis={landmark.visibility:.2f}, pos=({landmark.x:.2f}, {landmark.y:.2f}, {landmark.z:.2f})\")\n",
    "                \n",
    "                # If first frame with landmarks, initialize the skeleton hierarchy\n",
    "                if is_first_frame:\n",
    "                    print(\"First pose detected! Initializing skeleton...\")\n",
    "                    landmark_names = [f\"landmark_{i}\" for i in range(33)]\n",
    "                    skeleton.create_hierarchy(landmark_names)\n",
    "                    skeleton.update_joint_offsets(results.pose_landmarks.landmark)\n",
    "                    is_first_frame = False\n",
    "                    print(\"Skeleton initialized\")\n",
    "                \n",
    "                # Process frame for motion data\n",
    "                try:\n",
    "                    motion_data = skeleton.process_frame(results.pose_landmarks.landmark)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing motion data for frame {frame_count}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Draw the pose landmarks on the image if visualization is enabled\n",
    "                if visualize:\n",
    "                    # Draw pose landmarks\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        results.pose_landmarks,\n",
    "                        mp_pose.POSE_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                        mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=1)\n",
    "                    )\n",
    "                    \n",
    "                    # Add frame number\n",
    "                    cv2.putText(\n",
    "                        image, f\"Frame: {frame_count}/{total_frames}\", \n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2\n",
    "                    )\n",
    "                    \n",
    "                    # Display the image with landmarks\n",
    "                    cv2.imshow('MediaPipe Pose Detection', image)\n",
    "                    \n",
    "                    # Break loop on 'q' key press\n",
    "                    key = cv2.waitKey(5) & 0xFF\n",
    "                    if key == ord('q') or key == 27:  # 'q' or ESC\n",
    "                        print(\"Visualization stopped by user\")\n",
    "                        break\n",
    "                \n",
    "                # Add motion data to skeleton\n",
    "                for i, (name, joint) in enumerate(skeleton.joints.items()):\n",
    "                    # Initialize motion list if needed\n",
    "                    if not hasattr(joint, 'motion') or joint.motion is None:\n",
    "                        joint.motion = []\n",
    "                    \n",
    "                    # Calculate start index for this joint's data in the motion_data array\n",
    "                    start_idx = 0\n",
    "                    for prev_name, prev_joint in list(skeleton.joints.items())[:i]:\n",
    "                        start_idx += len(prev_joint.channels)\n",
    "                    \n",
    "                    # Extract relevant motion data for this joint\n",
    "                    channel_count = len(joint.channels)\n",
    "                    joint_data = motion_data[start_idx:start_idx + channel_count]\n",
    "                    \n",
    "                    # Ensure we have the right number of values\n",
    "                    if len(joint_data) < channel_count:\n",
    "                        joint_data.extend([0.0] * (channel_count - len(joint_data)))\n",
    "                    \n",
    "                    joint.motion.append(joint_data)\n",
    "            else:\n",
    "                if debug and frame_count % 10 == 0:\n",
    "                    print(f\"No pose detected in frame {frame_count}\")\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if visualize:\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    # Duration and statistics\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nProcessing complete: {processed_count} frames with pose data out of {frame_count} total frames\")\n",
    "    print(f\"Time taken: {duration:.2f} seconds ({frame_count/duration:.2f} FPS)\")\n",
    "    \n",
    "    # Check if we have any processed frames\n",
    "    if processed_count == 0:\n",
    "        print(\"\\nERROR: No frames were successfully processed!\")\n",
    "        print(\"Possible issues:\")\n",
    "        print(\"1. The video doesn't contain a clearly visible person\")\n",
    "        print(\"2. The lighting conditions make pose detection difficult\")\n",
    "        print(\"3. The person is too small in the frame or too far from the camera\")\n",
    "        print(\"4. The video format might be incompatible\")\n",
    "        print(\"\\nTry with a different video or with the person closer to camera\")\n",
    "        return False\n",
    "    \n",
    "    # Set the total number of frames in the BVH file\n",
    "    skeleton.frames = processed_count\n",
    "    \n",
    "    # Write BVH file\n",
    "    try:\n",
    "        print(f\"Writing BVH file to {output_file}...\")\n",
    "        skeleton.write_bvh(output_file)\n",
    "        print(f\"BVH file written successfully with {processed_count} frames of motion data\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing BVH file: {e}\")\n",
    "        return False\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description='Convert video to BVH motion file using MediaPipe.')\n",
    "#     parser.add_argument('input', help='Input video file')\n",
    "#     parser.add_argument('--output', help='Output BVH file (default: output.bvh)')\n",
    "#     parser.add_argument('--visualize', action='store_true', help='Show visualization during processing')\n",
    "#     parser.add_argument('--debug', action='store_true', help='Enable debug output')\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     # Set default output filename if not provided\n",
    "#     if not args.output:\n",
    "#         base_name = os.path.splitext(os.path.basename(args.input))[0]\n",
    "#         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         args.output = f\"{base_name}_{timestamp}.bvh\"\n",
    "    \n",
    "#     success = process_video(args.input, args.output, args.visualize, args.debug)\n",
    "    \n",
    "#     if success:\n",
    "#         print(\"\\nConversion completed successfully!\")\n",
    "#     else:\n",
    "#         print(\"\\nConversion failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780015ed-76f4-4bf3-9f8d-d4aba572d1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening video file: fight2.mp4\n",
      "Video info: 1280x720, 30.0 FPS, 43 frames\n",
      "Initializing MediaPipe pose detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740617367.792169   26298 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1740617367.794105   26397 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1740617367.859976   26375 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740617367.893540   26373 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740617367.914986   26377 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pose detected! Initializing skeleton...\n",
      "Skeleton initialized\n",
      "\n",
      "Processing complete: 43 frames with pose data out of 43 total frames\n",
      "Time taken: 1.94 seconds (22.17 FPS)\n",
      "Writing BVH file to fight2.bvh...\n",
      "BVH file written successfully with 43 frames of motion data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Convert video to BVH motion capture file\")\n",
    "# parser.add_argument(\"video_path\", help=\"Path to input video file\")\n",
    "# parser.add_argument(\"--output\", help=\"Path to output BVH file\")\n",
    "# parser.add_argument(\"--fps\", type=int, default=30, help=\"Target frames per second for BVH file\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# video_path = args.video_path\n",
    "# output_path = args.output\n",
    "\n",
    "# if not output_path:\n",
    "#     # Create output path if not specified\n",
    "#     video_name = Path(video_path).stem\n",
    "#     output_path = f\"{video_name}.bvh\"\n",
    "filename = \"fight2\"\n",
    "process_video(f\"{filename}.mp4\", f\"{filename}.bvh\", True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119c0f2-a481-4d0d-8545-2e06ea34e35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
