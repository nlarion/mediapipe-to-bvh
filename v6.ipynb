{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87e91e6-0760-4e3e-99c0-e1c96cc38de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 13:46:25.508623: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740606385.570709    8244 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740606385.588569    8244 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 13:46:25.747046: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class MediaPipeToBVH:\n",
    "    def __init__(self, fps: int = 30, scale: float = 100.0):\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.fps = fps\n",
    "        self.scale = scale\n",
    "        \n",
    "        # Define the BVH skeleton hierarchy\n",
    "        self.joint_hierarchy = {\n",
    "            \"Hips\": [\"Spine\", \"LeftUpLeg\", \"RightUpLeg\"],\n",
    "            \"Spine\": [\"Spine1\"],\n",
    "            \"Spine1\": [\"Neck\", \"LeftShoulder\", \"RightShoulder\"],\n",
    "            \"Neck\": [\"Head\"],\n",
    "            \"Head\": [\"End_Head\"],\n",
    "            \"LeftShoulder\": [\"LeftArm\"],\n",
    "            \"LeftArm\": [\"LeftForeArm\"],\n",
    "            \"LeftForeArm\": [\"LeftHand\"],\n",
    "            \"LeftHand\": [\"End_LeftHand\"],\n",
    "            \"RightShoulder\": [\"RightArm\"],\n",
    "            \"RightArm\": [\"RightForeArm\"],\n",
    "            \"RightForeArm\": [\"RightHand\"],\n",
    "            \"RightHand\": [\"End_RightHand\"],\n",
    "            \"LeftUpLeg\": [\"LeftLeg\"],\n",
    "            \"LeftLeg\": [\"LeftFoot\"],\n",
    "            \"LeftFoot\": [\"End_LeftFoot\"],\n",
    "            \"RightUpLeg\": [\"RightLeg\"],\n",
    "            \"RightLeg\": [\"RightFoot\"],\n",
    "            \"RightFoot\": [\"End_RightFoot\"]\n",
    "        }\n",
    "        \n",
    "        # MediaPipe landmark indices mapping to BVH joints\n",
    "        self.landmark_to_joint = {\n",
    "            \"Hips\": {\"indices\": [23, 24], \"weights\": [0.5, 0.5]},\n",
    "            \"Spine\": {\"indices\": [23, 24, 11, 12], \"weights\": [0.25, 0.25, 0.25, 0.25]},\n",
    "            \"Spine1\": {\"indices\": [11, 12], \"weights\": [0.5, 0.5]},\n",
    "            \"Neck\": {\"indices\": [11, 12], \"weights\": [0.5, 0.5]},\n",
    "            \"Head\": {\"indices\": [0], \"weights\": [1.0]},\n",
    "            \"LeftShoulder\": {\"indices\": [11], \"weights\": [1.0]},\n",
    "            \"LeftArm\": {\"indices\": [13], \"weights\": [1.0]},\n",
    "            \"LeftForeArm\": {\"indices\": [15], \"weights\": [1.0]},\n",
    "            \"LeftHand\": {\"indices\": [15, 17, 19], \"weights\": [0.2, 0.4, 0.4]},\n",
    "            \"RightShoulder\": {\"indices\": [12], \"weights\": [1.0]},\n",
    "            \"RightArm\": {\"indices\": [14], \"weights\": [1.0]},\n",
    "            \"RightForeArm\": {\"indices\": [16], \"weights\": [1.0]},\n",
    "            \"RightHand\": {\"indices\": [16, 18, 20], \"weights\": [0.2, 0.4, 0.4]},\n",
    "            \"LeftUpLeg\": {\"indices\": [23], \"weights\": [1.0]},\n",
    "            \"LeftLeg\": {\"indices\": [25], \"weights\": [1.0]},\n",
    "            \"LeftFoot\": {\"indices\": [27, 31], \"weights\": [0.7, 0.3]},\n",
    "            \"RightUpLeg\": {\"indices\": [24], \"weights\": [1.0]},\n",
    "            \"RightLeg\": {\"indices\": [26], \"weights\": [1.0]},\n",
    "            \"RightFoot\": {\"indices\": [28, 32], \"weights\": [0.7, 0.3]}\n",
    "        }\n",
    "        \n",
    "        # End site offsets (fixed lengths for end joints)\n",
    "        self.end_site_offsets = {\n",
    "            \"End_Head\": (0, 0, 15),      # Forward from head\n",
    "            \"End_LeftHand\": (-10, 0, 0),  # Left from hand\n",
    "            \"End_RightHand\": (10, 0, 0),  # Right from hand\n",
    "            \"End_LeftFoot\": (0, 0, 10),   # Forward from foot\n",
    "            \"End_RightFoot\": (0, 0, 10)   # Forward from foot\n",
    "        }\n",
    "        \n",
    "        # Initialize pose tracking\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            smooth_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "    def process_video(self, video_path, visualize=False):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file {video_path}\")\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        print(f\"Video has {total_frames} frames at {fps} FPS\")\n",
    "        \n",
    "        # Optional visualization video\n",
    "        out = None\n",
    "        if visualize:\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter('pose_tracking.mp4', fourcc, fps, (width, height))\n",
    "        \n",
    "        frames = []\n",
    "        frame_count = 0\n",
    "        last_good_landmarks = None\n",
    "        \n",
    "        while True:\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            # Progress indicator\n",
    "            if frame_count % max(1, total_frames // 10) == 0:\n",
    "                print(f\"Processing frame {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%)\")\n",
    "            \n",
    "            # Convert to RGB for MediaPipe\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = self.pose.process(image_rgb)\n",
    "            \n",
    "            if visualize and results.pose_landmarks:\n",
    "                # Draw pose landmarks on image\n",
    "                annotated_image = image.copy()\n",
    "                self.mp_drawing.draw_landmarks(\n",
    "                    annotated_image, \n",
    "                    results.pose_landmarks,\n",
    "                    self.mp_pose.POSE_CONNECTIONS\n",
    "                )\n",
    "                out.write(annotated_image)\n",
    "            \n",
    "            if results.pose_world_landmarks:\n",
    "                # Store landmarks\n",
    "                landmarks_dict = {}\n",
    "                for i, landmark in enumerate(results.pose_world_landmarks.landmark):\n",
    "                    landmarks_dict[i] = {\n",
    "                        'x': landmark.x,\n",
    "                        'y': landmark.y,\n",
    "                        'z': landmark.z,\n",
    "                        'visibility': landmark.visibility\n",
    "                    }\n",
    "                frames.append(landmarks_dict)\n",
    "                last_good_landmarks = landmarks_dict\n",
    "            else:\n",
    "                # If no pose detected, use last good landmarks\n",
    "                if last_good_landmarks:\n",
    "                    frames.append(last_good_landmarks.copy())\n",
    "                else:\n",
    "                    print(f\"Warning: No pose detected in frame {frame_count} and no previous landmarks available\")\n",
    "        \n",
    "        if visualize and out:\n",
    "            out.release()\n",
    "            \n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) == 0:\n",
    "            raise ValueError(\"No pose data detected in the video\")\n",
    "        \n",
    "        print(f\"Extracted pose data from {len(frames)} frames\")\n",
    "        return frames\n",
    "\n",
    "    def calculate_joint_positions(self, frames):\n",
    "        \"\"\"Calculate BVH joint positions from MediaPipe landmarks\"\"\"\n",
    "        joint_positions = []\n",
    "        \n",
    "        for frame_idx, landmarks in enumerate(frames):\n",
    "            frame_positions = {}\n",
    "            \n",
    "            # Calculate position for each joint\n",
    "            for joint_name, mapping in self.landmark_to_joint.items():\n",
    "                indices = mapping[\"indices\"]\n",
    "                weights = mapping[\"weights\"]\n",
    "                \n",
    "                # Calculate weighted average position\n",
    "                x = sum(landmarks[idx]['x'] * weights[i] for i, idx in enumerate(indices))\n",
    "                y = sum(landmarks[idx]['y'] * weights[i] for i, idx in enumerate(indices))\n",
    "                z = sum(landmarks[idx]['z'] * weights[i] for i, idx in enumerate(indices))\n",
    "                \n",
    "                # Convert coordinates from MediaPipe to Blender-friendly system\n",
    "                # MediaPipe: Y is up, X is right, Z toward camera\n",
    "                # Blender BVH: Z is up, X is right, Y is forward (away from camera)\n",
    "                frame_positions[joint_name] = {\n",
    "                    'x': x * self.scale,         # X remains the same\n",
    "                    'y': -z * self.scale,        # Y in Blender = -Z in MediaPipe\n",
    "                    'z': y * self.scale          # Z in Blender = Y in MediaPipe\n",
    "                }\n",
    "            \n",
    "            joint_positions.append(frame_positions)\n",
    "        \n",
    "        return joint_positions\n",
    "\n",
    "    def smooth_positions(self, positions, window_size=5):\n",
    "        \"\"\"Apply smoothing to reduce jitter\"\"\"\n",
    "        if len(positions) <= 1:\n",
    "            return positions\n",
    "            \n",
    "        smoothed = []\n",
    "        joint_names = positions[0].keys()\n",
    "        half_window = max(1, window_size // 2)\n",
    "        \n",
    "        for i in range(len(positions)):\n",
    "            smooth_frame = {}\n",
    "            \n",
    "            for joint in joint_names:\n",
    "                # Calculate window bounds\n",
    "                start = max(0, i - half_window)\n",
    "                end = min(len(positions), i + half_window + 1)\n",
    "                \n",
    "                # Calculate average position within window\n",
    "                x_sum = y_sum = z_sum = 0\n",
    "                count = 0\n",
    "                \n",
    "                for j in range(start, end):\n",
    "                    if joint in positions[j]:\n",
    "                        x_sum += positions[j][joint]['x']\n",
    "                        y_sum += positions[j][joint]['y']\n",
    "                        z_sum += positions[j][joint]['z']\n",
    "                        count += 1\n",
    "                \n",
    "                if count > 0:\n",
    "                    smooth_frame[joint] = {\n",
    "                        'x': x_sum / count,\n",
    "                        'y': y_sum / count,\n",
    "                        'z': z_sum / count\n",
    "                    }\n",
    "                else:\n",
    "                    smooth_frame[joint] = positions[i][joint].copy()\n",
    "            \n",
    "            smoothed.append(smooth_frame)\n",
    "        \n",
    "        return smoothed\n",
    "\n",
    "    def calculate_joint_offsets(self, reference_frame):\n",
    "        \"\"\"Calculate bone offsets from reference frame\"\"\"\n",
    "        offsets = {}\n",
    "        \n",
    "        for joint_name, children in self.joint_hierarchy.items():\n",
    "            if joint_name not in reference_frame:\n",
    "                continue\n",
    "                \n",
    "            parent_pos = reference_frame[joint_name]\n",
    "            \n",
    "            for child in children:\n",
    "                if child.startswith(\"End_\"):\n",
    "                    # Use predefined end site offset\n",
    "                    offsets[child] = self.end_site_offsets[child]\n",
    "                elif child in reference_frame:\n",
    "                    child_pos = reference_frame[child]\n",
    "                    offsets[child] = (\n",
    "                        child_pos['x'] - parent_pos['x'],\n",
    "                        child_pos['y'] - parent_pos['y'],\n",
    "                        child_pos['z'] - parent_pos['z']\n",
    "                    )\n",
    "        \n",
    "        return offsets\n",
    "\n",
    "    def calculate_rotations(self, positions):\n",
    "        \"\"\"Calculate joint rotations based on positions\"\"\"\n",
    "        all_rotations = []\n",
    "        \n",
    "        for frame_idx, frame_positions in enumerate(positions):\n",
    "            frame_rotations = {}\n",
    "            \n",
    "            # Process joints in hierarchy order\n",
    "            self._calculate_joint_rotations(frame_rotations, frame_positions, \"Hips\", [0, 0, 0])\n",
    "            \n",
    "            all_rotations.append(frame_rotations)\n",
    "            \n",
    "        return all_rotations\n",
    "\n",
    "    def _calculate_joint_rotations(self, rotations, positions, joint_name, parent_rotation):\n",
    "        \"\"\"Recursively calculate rotations for a joint and its children\"\"\"\n",
    "        # Skip end sites\n",
    "        if joint_name.startswith(\"End_\"):\n",
    "            return\n",
    "            \n",
    "        # Skip if joint not in positions\n",
    "        if joint_name not in positions:\n",
    "            rotations[joint_name] = [0, 0, 0]\n",
    "            return\n",
    "            \n",
    "        children = self.joint_hierarchy.get(joint_name, [])\n",
    "        \n",
    "        # Special case for the root (Hips)\n",
    "        if joint_name == \"Hips\":\n",
    "            # Hips rotation is global\n",
    "            rotations[joint_name] = [0, 0, 0]\n",
    "            \n",
    "            # Calculate rotation for children\n",
    "            for child in children:\n",
    "                if not child.startswith(\"End_\"):\n",
    "                    self._calculate_joint_rotations(rotations, positions, child, rotations[joint_name])\n",
    "            return\n",
    "        \n",
    "        # Calculate local rotation based on bone direction\n",
    "        x_rot = y_rot = z_rot = 0.0\n",
    "        \n",
    "        # Find first valid child to define bone direction\n",
    "        for child in children:\n",
    "            if child.startswith(\"End_\") or child not in positions:\n",
    "                continue\n",
    "                \n",
    "            # Calculate direction vector to child\n",
    "            dx = positions[child]['x'] - positions[joint_name]['x']\n",
    "            dy = positions[child]['y'] - positions[joint_name]['y']\n",
    "            dz = positions[child]['z'] - positions[joint_name]['z']\n",
    "            \n",
    "            length = math.sqrt(dx*dx + dy*dy + dz*dz)\n",
    "            if length < 0.001:\n",
    "                continue\n",
    "                \n",
    "            # Normalize direction\n",
    "            dx /= length\n",
    "            dy /= length\n",
    "            dz /= length\n",
    "            \n",
    "            # Convert direction to rotation angles\n",
    "            # These calculations need to be adjusted for Blender's coordinate system\n",
    "            \n",
    "            # X rotation (tilting forward/backward)\n",
    "            x_rot = math.degrees(math.atan2(-dy, dz))\n",
    "            \n",
    "            # Y rotation (turning left/right)\n",
    "            y_rot = math.degrees(math.atan2(dx, math.sqrt(dy*dy + dz*dz)))\n",
    "            \n",
    "            # Z rotation (tilting side to side)\n",
    "            z_rot = 0  # Simplified - most human joints don't have significant Z rotation\n",
    "            \n",
    "            # Joint-specific adjustments\n",
    "            if \"Arm\" in joint_name:\n",
    "                # Arms need additional rotation to point correctly\n",
    "                x_rot += 90\n",
    "            elif \"UpLeg\" in joint_name:\n",
    "                # Legs point downward\n",
    "                x_rot += 180\n",
    "            \n",
    "            break\n",
    "        \n",
    "        rotations[joint_name] = [x_rot, y_rot, z_rot]\n",
    "        \n",
    "        # Calculate rotation for children\n",
    "        for child in children:\n",
    "            if not child.startswith(\"End_\"):\n",
    "                self._calculate_joint_rotations(rotations, positions, child, rotations[joint_name])\n",
    "\n",
    "    def find_tpose_frame(self, positions):\n",
    "        \"\"\"Try to find a T-pose frame in the sequence\"\"\"\n",
    "        best_idx = 0\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        # Check first few frames, they often contain initialization pose\n",
    "        check_frames = min(30, len(positions))\n",
    "        \n",
    "        for i in range(check_frames):\n",
    "            # Calculate T-pose score based on arm extension and symmetry\n",
    "            if \"LeftShoulder\" not in positions[i] or \"RightShoulder\" not in positions[i]:\n",
    "                continue\n",
    "                \n",
    "            left_arm_hor = 0\n",
    "            right_arm_hor = 0\n",
    "            \n",
    "            # Check horizontal arm extension\n",
    "            if \"LeftHand\" in positions[i] and \"LeftShoulder\" in positions[i]:\n",
    "                dx = positions[i][\"LeftHand\"][\"x\"] - positions[i][\"LeftShoulder\"][\"x\"]\n",
    "                dy = positions[i][\"LeftHand\"][\"y\"] - positions[i][\"LeftShoulder\"][\"y\"]\n",
    "                left_arm_hor = abs(dx) - abs(dy)  # Higher is better (more horizontal)\n",
    "                \n",
    "            if \"RightHand\" in positions[i] and \"RightShoulder\" in positions[i]:\n",
    "                dx = positions[i][\"RightHand\"][\"x\"] - positions[i][\"RightShoulder\"][\"x\"]\n",
    "                dy = positions[i][\"RightHand\"][\"y\"] - positions[i][\"RightShoulder\"][\"y\"]\n",
    "                right_arm_hor = abs(dx) - abs(dy)  # Higher is better (more horizontal)\n",
    "            \n",
    "            # Check if shoulders are level\n",
    "            shoulder_level = 0\n",
    "            if \"LeftShoulder\" in positions[i] and \"RightShoulder\" in positions[i]:\n",
    "                shoulder_level = -abs(positions[i][\"LeftShoulder\"][\"z\"] - positions[i][\"RightShoulder\"][\"z\"])\n",
    "            \n",
    "            # Check if arms are extended outward (not forward)\n",
    "            arm_extension = 0\n",
    "            if \"LeftHand\" in positions[i] and \"LeftShoulder\" in positions[i]:\n",
    "                arm_extension += abs(positions[i][\"LeftHand\"][\"x\"] - positions[i][\"LeftShoulder\"][\"x\"])\n",
    "            if \"RightHand\" in positions[i] and \"RightShoulder\" in positions[i]:\n",
    "                arm_extension += abs(positions[i][\"RightHand\"][\"x\"] - positions[i][\"RightShoulder\"][\"x\"])\n",
    "            \n",
    "            # Calculate overall T-pose score\n",
    "            score = left_arm_hor + right_arm_hor + shoulder_level + arm_extension\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_idx = i\n",
    "        \n",
    "        return best_idx\n",
    "\n",
    "    def write_bvh_file(self, positions, rotations, output_path):\n",
    "        \"\"\"Write BVH file from joint positions and rotations\"\"\"\n",
    "        if len(positions) < 2:\n",
    "            raise ValueError(\"Need at least 2 frames to create animation\")\n",
    "            \n",
    "        # Find the best reference frame for skeleton\n",
    "        ref_idx = self.find_tpose_frame(positions)\n",
    "        reference_frame = positions[ref_idx]\n",
    "        print(f\"Using frame {ref_idx} as reference for skeleton\")\n",
    "        \n",
    "        # Calculate joint offsets from reference frame\n",
    "        offsets = self.calculate_joint_offsets(reference_frame)\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            # Write HIERARCHY section\n",
    "            f.write(\"HIERARCHY\\n\")\n",
    "            f.write(\"ROOT Hips\\n\")\n",
    "            f.write(\"{\\n\")\n",
    "            f.write(\"\\tOFFSET 0.00 0.00 0.00\\n\")\n",
    "            f.write(\"\\tCHANNELS 6 Xposition Yposition Zposition Zrotation Xrotation Yrotation\\n\")\n",
    "            \n",
    "            # Write joint hierarchy\n",
    "            self._write_joint_hierarchy(f, \"Hips\", offsets, 1)\n",
    "            \n",
    "            # End HIERARCHY section\n",
    "            f.write(\"}\\n\")\n",
    "            \n",
    "            # Write MOTION section\n",
    "            f.write(\"MOTION\\n\")\n",
    "            f.write(f\"Frames: {len(positions)}\\n\")\n",
    "            f.write(f\"Frame Time: {1.0/self.fps:.6f}\\n\")\n",
    "            \n",
    "            # Write frame data\n",
    "            for i in range(len(positions)):\n",
    "                line = []\n",
    "                \n",
    "                # Root position (Hips) - keep in Blender-friendly coordinate system\n",
    "                hips = positions[i][\"Hips\"]\n",
    "                line.extend([hips['x'], hips['y'], hips['z']])\n",
    "                \n",
    "                # Add joint rotations\n",
    "                for joint in self._get_joint_list(\"Hips\"):\n",
    "                    if joint in rotations[i]:\n",
    "                        # BVH uses ZXY rotation order\n",
    "                        # Reorder from our internal XYZ to BVH's ZXY\n",
    "                        x_rot = rotations[i][joint][0]\n",
    "                        y_rot = rotations[i][joint][1]\n",
    "                        z_rot = rotations[i][joint][2]\n",
    "                        line.extend([z_rot, x_rot, y_rot])\n",
    "                    else:\n",
    "                        # No rotation data, use zeros\n",
    "                        line.extend([0.0, 0.0, 0.0])\n",
    "                \n",
    "                # Write frame line\n",
    "                f.write(\" \".join(f\"{val:.6f}\" for val in line) + \"\\n\")\n",
    "        \n",
    "        print(f\"BVH file written to {output_path}\")\n",
    "\n",
    "    def _write_joint_hierarchy(self, file, joint_name, offsets, indent_level):\n",
    "        \"\"\"Write joint hierarchy to BVH file\"\"\"\n",
    "        indent = \"\\t\" * indent_level\n",
    "        \n",
    "        for child in self.joint_hierarchy.get(joint_name, []):\n",
    "            if child.startswith(\"End_\"):\n",
    "                # Write end site\n",
    "                file.write(f\"{indent}End Site\\n\")\n",
    "                file.write(f\"{indent}{{\\n\")\n",
    "                \n",
    "                offset = offsets.get(child, (0, 0, 0))\n",
    "                file.write(f\"{indent}\\tOFFSET {offset[0]:.6f} {offset[1]:.6f} {offset[2]:.6f}\\n\")\n",
    "                \n",
    "                file.write(f\"{indent}}}\\n\")\n",
    "            else:\n",
    "                # Write child joint\n",
    "                file.write(f\"{indent}JOINT {child}\\n\")\n",
    "                file.write(f\"{indent}{{\\n\")\n",
    "                \n",
    "                offset = offsets.get(child, (0, 0, 0))\n",
    "                file.write(f\"{indent}\\tOFFSET {offset[0]:.6f} {offset[1]:.6f} {offset[2]:.6f}\\n\")\n",
    "                \n",
    "                # All non-root joints have rotation only\n",
    "                file.write(f\"{indent}\\tCHANNELS 3 Zrotation Xrotation Yrotation\\n\")\n",
    "                \n",
    "                # Write child's children\n",
    "                self._write_joint_hierarchy(file, child, offsets, indent_level + 1)\n",
    "                \n",
    "                file.write(f\"{indent}}}\\n\")\n",
    "\n",
    "    def _get_joint_list(self, start_joint):\n",
    "        \"\"\"Get a flat list of all joints in hierarchy order\"\"\"\n",
    "        joints = [start_joint]\n",
    "        \n",
    "        for child in self.joint_hierarchy.get(start_joint, []):\n",
    "            if not child.startswith(\"End_\"):\n",
    "                joints.extend(self._get_joint_list(child))\n",
    "        \n",
    "        return joints\n",
    "\n",
    "    def convert_video_to_bvh(self, video_path, output_path, visualize):\n",
    "        \"\"\"Convert video to BVH file\"\"\"\n",
    "        print(f\"Processing video: {video_path}\")\n",
    "        \n",
    "        # Extract pose data from video\n",
    "        frames = self.process_video(video_path, visualize)\n",
    "        \n",
    "        # Calculate joint positions for each frame\n",
    "        print(\"Calculating joint positions...\")\n",
    "        positions = self.calculate_joint_positions(frames)\n",
    "        \n",
    "        # Apply smoothing to reduce jitter with smaller window\n",
    "        print(\"Smoothing motion...\")\n",
    "        smoothed_positions = self.smooth_positions(positions, window_size=3)\n",
    "        \n",
    "        # Calculate joint rotations\n",
    "        print(\"Calculating joint rotations...\")\n",
    "        rotations = self.calculate_rotations(smoothed_positions)\n",
    "        \n",
    "        # Write BVH file\n",
    "        print(f\"Writing BVH file to: {output_path}\")\n",
    "        self.write_bvh_file(smoothed_positions, rotations, output_path)\n",
    "        \n",
    "        print(\"Conversion complete!\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser(description='Convert video to BVH using MediaPipe.')\n",
    "#     parser.add_argument('--input', type=str, required=True, help='Input video file')\n",
    "#     parser.add_argument('--output', type=str, help='Output BVH file')\n",
    "#     parser.add_argument('--fps', type=int, default=30, help='Frames per second for BVH')\n",
    "#     parser.add_argument('--scale', type=float, default=100.0, help='Scale factor for the skeleton')\n",
    "#     parser.add_argument('--visualize', action='store_true', help='Save visualization of pose tracking')\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     # If output path is not specified, use input filename with .bvh extension\n",
    "#     if not args.output:\n",
    "#         base_name = os.path.splitext(os.path.basename(args.input))[0]\n",
    "#         args.output = f\"{base_name}.bvh\"\n",
    "    \n",
    "#     try:\n",
    "#         converter = MediaPipeToBVH(fps=args.fps, scale=args.scale)\n",
    "#         converter.convert_video_to_bvh(args.input, args.output, args.visualize)\n",
    "#         print(f\"âœ“ Successfully created BVH file: {args.output}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {str(e)}\")\n",
    "#         return 1\n",
    "    \n",
    "#     return 0\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9770f0eb-f73e-439a-883e-5bbc7d3fd852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740606399.739386    8244 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1740606399.741890    8840 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1740606399.812250    8827 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740606399.895882    8820 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: dance2.mp4\n",
      "Video has 245 frames at 30.0 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740606399.970183    8819 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame 24/245 (9.8%)\n",
      "Processing frame 48/245 (19.6%)\n",
      "Processing frame 72/245 (29.4%)\n",
      "Processing frame 96/245 (39.2%)\n",
      "Processing frame 120/245 (49.0%)\n",
      "Processing frame 144/245 (58.8%)\n",
      "Processing frame 168/245 (68.6%)\n",
      "Processing frame 192/245 (78.4%)\n",
      "Processing frame 216/245 (88.2%)\n",
      "Processing frame 240/245 (98.0%)\n",
      "Extracted pose data from 245 frames\n",
      "Calculating joint positions...\n",
      "Smoothing motion...\n",
      "Calculating joint rotations...\n",
      "Writing BVH file to: dance2.bvh\n",
      "Using frame 20 as reference for skeleton\n",
      "BVH file written to dance2.bvh\n",
      "Conversion complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"dance2\"\n",
    "converter = MediaPipeToBVH(fps=30, scale=100.0)\n",
    "converter.convert_video_to_bvh(f\"{filename}.mp4\", f\"{filename}.bvh\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522a83f-a88c-450e-8598-2db1f13c99ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
